\section{Appendix}

\subsection{Microbiota analysis}

\begin{proposition}(Bernoulli tree prior)
    \label{proposition:bernoulli_tree_prior}
    \\
    Let $T = (T^{(1)}, \dots, T^{(L)})$ a random tree of depth $L$. \\
    For all $\ell \in \{1, \dots, L\}$, denote $T^{(\ell)} = (u_1^{(\ell)}, \dots, u_{K_{\ell}}^{(\ell)})$ the nodes at layer $\ell$,
    such that $u_k^{(\ell)} \in \{0, 1\}$ denotes whether or not the node is activated (1 if activated). \\
    Denote $\mathcal{P}(u_k^{(\ell)})$ the parent of the node $u_k^{(\ell)}$ that outputs $1$ if the parent exists and is activated, $0$ otherwise. \\
    Assume that:
    \begin{itemize}
        \item $p(T^{(1)}) = \delta_{e_1}(T^{(1)})$
        \item $\forall l \geq 2, p(T^{(\ell+1)} | T^{(1:\ell)}) = p(T^{(\ell+1)} | T^{(\ell)})$
        \item $\forall l \geq 2, k \in \{1, \dots, K_{\ell}\}, u_k^{(\ell)} | \left\{\mathcal{P}(u_k^{(\ell)}) = 1 \right\} \sim \mathcal{B}(\pi_k^{(\ell)}) \right$
        \item $\displaystyle p\left(u_1^{(\ell)}, \dots, u_{K_{\ell}}^{(\ell)} | \mathcal{P}(u_1^{(\ell)}), \mathcal{P}(u_{K_{\ell}}^{(\ell)})\right) = \prod_{k=1}^{K_{\ell}} p\left(u_k^{(\ell)} | \mathcal{P}(u_k^{(\ell)})\right)$
    \end{itemize}

    Then, the distribution of $T$ can be written as:
    $$
    p(T) = \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left(\left(\pi_k^{(\ell+1)}\right)^{u_k^{(\ell+1)}} \left(1-\pi_k^{(\ell+1)}\right)^{1 - u_k^{(\ell+1)}} \right)^{\mathcal{P}(u_k^{(\ell+1)})}
    $$
\end{proposition}

\begin{proof}
    $$
    \begin{align}
        p(T) &= p\left(T^{(1)}, \dots, T^{(L)}\right) \\
        &= p\left(T^{(1)}\right) \prod_{l=1}^{L-1} p\left(T^{(\ell+1)}|T^{(\ell)}\right) \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} p\left(u_1^{(\ell+1)}, \dots, u_{K_{\ell}}^{(\ell+1)} | \nodeparent(u_1^{(\ell+1)}), \dots, \nodeparent(u_{K_{\ell}}^{(\ell+1)})\right) \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} p\left(u_k^{(\ell+1)} | \nodeparent(u_k^{(\ell+1)}) \right) \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left( \pi_k^{(\ell+1)} u_k^{(\ell+1)} + \left(1 - \pi_k^{(\ell+1)} \right)(1 - u_k^{(\ell+1)}) \right)^{\nodeparent(u_k^{(\ell+1)})} \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left( \left(\pi_k^{(\ell+1)}\right)^{u_k^{(\ell+1)}} \left(1 - \pi_k^{(\ell+1)} \right)^{1 - u_k^{(\ell+1)}} \right)^{\nodeparent(u_k^{(\ell+1)})}
    \end{align}
    $$
\end{proof}

\begin{proposition}[MLE of the Bernoulli tree prior]
    \label{proposition:mle_bernoulli_tree_prior}
    \\
    Recall the context of \ref{proposition:bernoulli_tree_prior}. \\
    Let $(T_1, \dots, T_n)$ $n$ trees i.i.d following the bernoulli tree prior.
    Denote the maximum likelihood objective by
    $$
    \begin{equation}
        \begin{align}
            arg \max_{\pi_j^{(m)}} \quad & \sum_{i=1}^n \sum_{l=1}^{L-1} \sum_{k=1}^{K_{\ell}} \nodeparent(u_{k,i}^{(\ell+1)}) \left[u_{k,i}^{(\ell+1)} \log \pi_{k}^{(\ell+1)} + (1 - u_{k,i}^{(\ell+1)}) \log (1 - \pi_k^{(\ell+1)}) \right] \\
            \textrm{s.t.} \quad & \forall k,l, \pi_{k}^{(\ell)} \in [0, 1] \\
        \end{align}
        \label{eq:prior_transition_objective}
    \end{equation}
    $$

    Then the maximum likelihood estimator is given by
    $$
    \left(\pi_k^{(\ell)}\right)^* = \frac{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)}) u_{k,i}^{(\ell)}}{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)})}
    $$
\end{proposition}

\begin{proof}
    Simply by deriving the objective we obtain:
    $$
    \begin{align}
        \partial_{\pi_j^{(m)}} \log p(T_1, \dots, T_n) &= \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) \left[ u_{j,i}^{(m)} \frac{1}{\pi_j^{(m)}} + (1 - u_{j,i}^{(m)}) \frac{-1}{1-\pi_j^{(m)}}\right] \\
        &= \frac{1}{\pi_j^{(m)}} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)} - \frac{1}{1 - \pi_j^{(m)}} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) (1 - u_{j,i}^{(m)})
    \end{align}
    $$

    Looking for $0$ valued gradient, we end up with:
    $$
    \begin{align}
    (1 - \pi_j^{(m)}) \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)} - \pi_j^{(m)} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) (1 - u_{j,i}^{(m)}) &= 0 \\
    \pi_j^{(m)} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) &= \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)} \\
    \pi_j^{(m)} &= \frac{\sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)}}{\sum_{i=1}^n \nodeparent(u_{j,i}^{(m)})}
    \end{align}
    $$

    Since the obtained value respects the constraint, that concludes the proof.
\end{proof}

\begin{proposition}[Law of $aX$]
    \label{proposition:law_of_aX}
    \\
    Let $X \sim f(x)$ a random variable with density $f$. \\
    Let $a \in (0, 1)$. \\
    The distribution of $aX$ is given by:
    $$p_{aX}(u) = \frac{1}{a} f\left(\frac{u}{a}\right)$$
\end{proposition}

\begin{proof}
    We use the transfer theorem.
    Let $g$ a continuous and measurable function.
    $$
    \begin{align}
        \mathbb{E}[g(aX)] &= \int g(ax) f(x) dx \\
                          &= \int g(u) f\left(\frac{u}{a}\right) \frac{1}{a} du
    \end{align}
    $$
    Hence, one can identify the distribution of $aX$ as $p_{aX}(u) = \frac{1}{a} f\left(\frac{u}{a}\right)$
\end{proof}

\begin{proposition}[Abundance distribution conditionally to the trees]
    \label{proposition:abundance_posterior_bernoulli_tree}

    \newline

    Let $X = (X^{(1)}, \dots, X^{(L)})$ the abundance matrix of a tree $T$ with depth $L$. \\
    For all $\ell \in \{1, \dots, L\}$, denote $X^{(\ell)} = (x_1^{(\ell)}, \dots, x_{K_{\ell}}^{(\ell)})$ the abundance value of each node of the tree. \\
    Denote $\childrennode(x_k^{(\ell)})$ the vector of children abundances related to $x_k^{(\ell)}$, which is empty if it has no children. \\
    Denote $T^{(\ell + 1)}_k$ the vector of activation states $(u_j^{(\ell+1)})_j$ of the children of the node $u_k^{(l)}$ in $T$. \\
    Assume that:
    \begin{itemize}
        \item $p(X^{(1)}) = \delta_{e_1}(X^{(1)})$
        \item $p(X^{(\ell+1)} | X^{(1:\ell)}, T) = p(X^{(\ell+1)} | X^{(\ell)}, T^{(\ell+1)})$
        \item $p(X^{(\ell+1)} | X^{(\ell)}, T^{(\ell+1)}) = \displaystyle\prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, T^{(\ell+1)})$
        \item For all $l \in \{2, \dots, L\}, k \in \{1, \dots, K_{\ell}\}$,
              \begin{itemize}
                  \item If $|\childrennode(x_k^{(\ell)})| > 1$: $\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, T^{(\ell+1)} \sim x_k^{(\ell)} D(\alpha_k^{(\ell)} \odot T^{(\ell+1)}_k)$
                  \item If $|\childrennode(x_k^{(\ell)})| = 1$, $\childrennode(x_k^{(\ell)}) |  x_k^{(\ell)}, T^{(\ell+1)} \sim \delta_{x_k^{(\ell)}}\left(\childrennode(x_k^{(\ell)})\right)$
              \end{itemize}
    \end{itemize}

    Then, the distribution of $X$ conditionally to $T$ is given by:
    $$
        p(X|T) = \delta_{e_1}(X^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left(
    \delta_{x_k^{(\ell)}}\left(\childrennode(x_k^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| = 1}}
    \frac{1}{x_k^{(\ell)}} f_{\alpha_k^{(\ell)} \odot T^{(\ell+1)}_k} \left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right)^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| > 1}}
                \right)
    $$
    
\end{proposition}

\begin{proof}
    $$
    \begin{align}
        p(X | T) &= p(X^{(1)}, \dots, X^{(L)} | T) \\
                &= p(X^{(1)} | T^{(1)}) \prod_{l=1}^{L-1} p(X^{(\ell+1)} | X^{(\ell)}, T^{({\ell}+1)}) \\
                &= \delta_{e_1}(X^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, T^{(\ell+1)}) \\
                &= \delta_{e_1}(X^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left(
                                                                                \delta_{x_k^{(\ell)}}\left(\childrennode(x_k^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| = 1}}
                                                                                \left[\underbrace{\frac{1}{x_k^{(\ell)}} f_{\alpha_k^{(\ell)} \odot T^{(\ell+1)}_k} \left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right)}_{\text{proposition \ref{proposition:law_of_aX}}}\right]^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| > 1}}
                                                                            \right)
    \end{align}
    $$
\end{proof}

\begin{proposition}[MLE of the abundance distribution a posteriori]
    \label{MLE_abundance_bernoulli_tree}
    \\
    Consider the context of proposition \ref{proposition:abundance_posterior_bernoulli_tree}. \\
    We consider a data set $(X_i, T_i)_{1 \geq i \geq n}$ of abundance matrix and trees. \\
    Denote by $\mathcal{V}(\alpha_k^{(\ell)}, T) = \left\{v \in \mathbb{N} | \left[ \alpha_k^{(\ell)} \odot T^{(\ell+1)}_k \right]_v \neq 0\right\}$
    the set of indexes so that the coordinate of $\alpha_k^{(\ell)}$ is not masked by $T^{(\ell+1)}$. \\
    The maximum likelihood estimator of the distribution characterising the abundance conditionally to the tree is then
    given by the following fixed point algorithm: \\

    $
    \forall l \in \{1, \dots, L\}, k \in \{1, \dots, K_{\ell}\}, v \in \{1, \dots, |\alpha_k^{(\ell)}|\}
    $,

    $$
    \alpha_{k,v}^{(\ell)} \leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)}\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)}} \right) \right)
    $$
\end{proposition}

\begin{proof}
    The maximum likelihood objective can be written as:
    $$
    \begin{align*}
        arg \max_{\alpha_{j,v}^{(m)}} \quad & \sum_{i=1}^n \sum_{l=1}^{L-1} \sum_{k=1}^{K_{\ell}} \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \log f_{\alpha_k^{(\ell)} \odot T_{k,i}^{(\ell+1)}} \left(\frac{C(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) \\
    \end{align*}
    $$
    Using the Dirichlet distribution expression, we can write the following:
    \small
    $$
    \begin{align}
        \log f_{\alpha_k^{(\ell)} \odot T_{k,i}^{(\ell+1)}}\left(\frac{C(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) = \log \Gamma \left(\sum_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,v}^{(\ell)}\right)
        - \sum_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \Gamma \left( \alpha_{k,v}^{(\ell)} \right)
        + \sum_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left(\alpha_{k,v}^{(\ell)} - 1\right) \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}}
    \end{align}
    $$
    \normalsize

    Writing $\psi$ the digamma function, the derivative of the objective relatively to a fixed $\alpha_{k,v}^{(\ell)}$ is given by:
    $$
    \partial_{\alpha_{k,v}^{(\ell)}} \log p(X|T) = \sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[\psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)} \right) - \psi\left(\alpha_{k,v}^{(\ell)} \right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}}\right]
    $$

    Looking for $0$ valued gradient, we obtain the following equation:
    $$
    \psi\left(\alpha_{k,v}^{(\ell)}\right) = \frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)}\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]}
                                                {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)}} \right)
    $$

    Using the reference trick from \cite{dirichlet_digamma_trick}, one can compute an iterative fixed point algorithm to solve the previous equation,
    leading to:
    $$
    \alpha_{k,v}^{(\ell)} \leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)}\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)}} \right) \right)
    $$
\end{proof}


\begin{proposition}[Expectation-Maximization for Taxonomy Clustering model]
    \label{proposition:EM_taxonomy_clustering}
    \newline
    We use the same context as for proposition \ref{proposition:abundance_posterior_bernoulli_tree} of this section.
    Assume the following framework changes:
    \begin{itemize}
        \item $(X_i, T_i)_{1 \le i \le n}$ samples of taxa-abundance data ($X_i$ abundance, $T_i$ associated taxonomy)
        \item Let $Z_i$ a latent variable taking values in $\{1, \dots, C\}$.
        \item We model the distributions as follow:
            $$
            \begin{align}
                p(Z_i = c) &= \gamma_c \\
                p\left(u_{k,i}^{(\ell)} = 1 | \nodeparent(u_{k,i}^{(\ell)}) = 1, Z_i = c\right) &= \pi_{c,k}^{(\ell)} \\
                p\left(\childrennode(x_{k,i}^{(\ell)}) | x_{k,i}^{(\ell)}, T^{(\ell+1)}, Z_i = c \right) &\sim x_k^{(\ell)} \mathcal{D}(\alpha_{c,k}^{(\ell)} \odot T_k^{(\ell+1)}) \quad \text{if } |\childrennode(x_k^{(\ell)})| > 1
            \end{align}
            $$
    \end{itemize}

    Consider the optimization problem:
    $$
    \theta^* = arg\max_{\theta} \mathbb{E}_{p_{\widehat{\theta}}}[\log p_{\theta}(X, T, Z) | X, T]
    $$

    Then, the Expectation-Maximization algorithm solving that problem is given by (for each sample $i$ and cluster index $c$): \\

    \textbf{Expectation step}:

        $$
        p_{\widehat{\theta}}(Z_i = c | X_i, T_i) = \tau_{ic} =
                        \frac{\widehat{\gamma}_c p_{\widehat{\theta}}(T_i | Z_i = c) p_{\widehat{\theta}}(X_i | T_i, Z_i = c)
                             }
                             {
                                \sum_{c'=1}^C
                                \widehat{\gamma}_{c'} p_{\widehat{\theta}}(T_i | Z_i = c') p_{\widehat{\theta}}(X_i | T_i, Z_i = c')
                             }
        $$

        where:
        $$
        \begin{align}
            &p_{\widehat{\theta}}(T_i | Z_i = c) = \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \left[\left(\widehat{\pi}_{c,k}^{(\ell+1)}\right)^{u_{k,i}^{(\ell+1)}} \left(1 - \widehat{\pi}_{c,k}^{(\ell+1)}\right)^{1 - u_{k,i}^{(\ell + 1)}}\right]^{\nodeparent(u_{k,i}^{(\ell+1)})} \\
            &p_{\widehat{\theta}}(X_i | T_i, Z_i = c) = \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \delta_{x_{k,i}^{(\ell)}}\left(\childrennode(x_{k,i}^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| = 1}} \left(\frac{1}{x_{k,i}^{(\ell)}} f_{\widehat{\alpha}_{c,k}^{(\ell)} \odot T_{i,k}^{(\ell+1)}}\left(\frac{\childrennode(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) \right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1}}
        \end{align}
        $$

    \medskip
    \textbf{Maximization step}:

    \medskip

    $\displaystyle
    \gamma_c^*  = \frac{1}{n} \sum_{i=1}^n \tau_{ic}
    $

    $\displaystyle
        \left(\pi_{c,k}^{(\ell)}\right)^* &= \frac{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)}) u_{k,i}^{(\ell)} \tau_{ic}}{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)}) \tau_{ic}} \\
    $

    $\displaystyle
    \left[\alpha_{c,k}^{(\ell)}\right]_v &\leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[\alpha_{c,k}^{(\ell)}\right]_u\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]\tau_{ic}}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \tau_{ic}} \right) \right)
    $
\end{proposition}

\begin{proof}

    We study the two parts of the EM algorithm, starting with the expectation step.

    \medskip

    \textbf{Expectation step}:

    \medskip

    Applying Bayes formula, we obtain:
    $$
    \begin{align}
        p_{\widehat{\theta}}(Z_i = c | X_i, T_i) &= \tau_{ic} \\
                              &= \frac{p_{\widehat{\theta}}(Z_i = c, X_i, T_i)}{\sum_{c'=1}^C p_{\widehat{\theta}}(Z_i = c', X_i, T_i)} \\
                              &= \frac{\widehat{\gamma}_c p_{\widehat{\theta}}(T_i | Z_i = c) p_{\widehat{\theta}}(X_i | T_i, Z_i = c)}{\sum_{c'=1}^C \widehat{\gamma}_{c'} p_{\widehat{\theta}}(T_i = Z_i = c') p_{\widehat{\theta}}(X_i | T_i, Z_i = c')}
    \end{align}
    $$

    Exploiting the model framework, we obtain the following expression for the conditional distribution of the taxonomy:
    $$
    \begin{align}
        p_{\theta}(T_i | Z_i = c) &= p_{\theta}(T_i^{(1)}, \dots, T_i^{(L)} | Z_i = c) \\
                        &= \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} p_{\theta}(T_i^{(\ell + 1)} | T_i^{(\ell)}, Z_i = c) \\
                        &= \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} p_{\theta}(u_{k,i}^{(\ell+1)} | \nodeparent(u_{k,i}^{(\ell+1)}), Z_i = c) \\
                        &= \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \left[\left(\pi_{c,k}^{(\ell+1)}\right)^{u_{k,i}^{(\ell+1)}} \left(1 - \pi_{c,k}^{(\ell+1)}\right)^{1 - u_{k,i}^{(\ell + 1)}}\right]^{\nodeparent(u_{k,i}^{(\ell+1)})}
    \end{align}
    $$

    Similarly, the conditional abundance distribution is given by:

    $$
    \begin{align}
        p_{\theta}(X_i | T_i, Z_i = c) &= p_{\theta}(X_i^{(1)}, \dots, X_i^{(L)} | T_i, Z_i = c) \\
                                        &= \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} p_{\theta}(X_i^{(\ell+1)} | X_i^{(\ell)}, T_i^{(\ell+1)}, Z_i = c) \\
                                        &= \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} p_{\theta}(\childrennode(x_{k,i}^{(\ell)}) | x_{k,i}^{(\ell)}, T_i^{(\ell+1)}, Z_i=c) \\
                                        &= \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \delta_{x_{k,i}^{(\ell)}}\left(\childrennode(x_{k,i}^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| = 1}} \left(\frac{1}{x_{k,i}^{(\ell)}} f_{\alpha_{c,k}^{(\ell)} \odot T_{i,k}^{(\ell+1)}}\left(\frac{\childrennode(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) \right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1}}\\
    \end{align}
    $$

    \medskip

    \textbf{Maximization step}

    \medskip

    Recall the general objective:
    $$
    \begin{align}
        \theta^* &= arg\max_{\theta} Q(\widehat{\theta}, \theta) \\
                 &= arg\max_{\theta} \frac{1}{n} \sum_{i=1}^n \sum_{c=1}^C \left(\log \gamma_c + \log p(T_i | Z_i = c) + \log p(X_i | T_i, Z_i = c)\right) \tau_{ic}
    \end{align}
    $$

    We start by optimizing $\gamma$.
    The objective can be written as:
    $$
    \begin{equation}
        \begin{align}
            arg \max_{\gamma_j} \quad & \sum_{i=1}^n \sum_{c=1}^{C} \tau_{ic} \log \gamma_c  \\
            \textrm{s.t.} \quad & \sum_{c=1}^{C} \gamma_c = 1 \\
                                & \forall c, \gamma_c \geq 0
        \end{align}
        \label{eq:objective_latent_gamma_c_microbiota_clustering}
    \end{equation}
    $$

    This optimization problem can be solved using Lagrange duality.
    KKT conditions are then written as (optimizing relatively to any $\gamma_c$):
    $$
    \begin{align}
        &\frac{1}{\gamma_c} \sum_{i=1}^n \tau_{ic} + \phi - \psi = 0 \\
        &\sum_{c=1}^C \gamma_c = 1 \\
        &\psi \geq 0 \\
        &\forall c, \gamma_c \geq 0 \\
        &\psi \gamma_c = 0
    \end{align}
    $$

    Using the first identity, we get:
    $$
    \gamma_c = \frac{1}{\psi - \phi} \sum_{i=1}^n \tau_{ic}
    $$

    Let's set $\phi = 0$, which enables use to ensure the last identity.
    Using the second identity, we then have:
    $$
    \begin{align}
        \sum_{c=1}^C \gamma_c = 1 &= \sum_{c=1}^C \frac{1}{\psi} \sum_{i=1}^n \tau_{ic} \\
                                    &= \frac{1}{\psi} \sum_{i=1}^n \sum_{c=1}^C \tau_{ic} \\
                                    &= \frac{1}{\psi} \sum_{i=1}^n 1 \\
                                    &= \frac{n}{\psi}
    \end{align}
    $$

    Hence, we get $\psi = n$, which leads us to the optimal $\gamma_c$ that verifies the KKT constraints:
    $$
    \gamma_c^* = \frac{1}{n} \sum_{i=1}^n \tau_{ic}
    $$

    Looking at $\pi_c$ now, the optimization objective can be written as:
    $$
    \begin{equation}
        \begin{align}
            arg \max_{\pi_{c,k}^{(\ell)}} \quad & \sum_{i=1}^n \sum_{c=1}^{C} \tau_{ic} \log p_{\pi_c}(T_i | Z_i = c)  \\
            \textrm{s.t.} \quad & \pi_{c,k}^{(\ell)} \in [0, 1]
        \end{align}
    \end{equation}
    $$

    This objective is very similar to the MLE computed for proposition \ref{proposition:mle_bernoulli_tree_prior}, up to
    a multiplication by $\tau_{ic}$.
    Hence, we refer to the proposition \ref{proposition:mle_bernoulli_tree_prior} for the computation, which leads to the following
    optimal $\pi_c$:
    $$
    \left(\pi_{c,k}^{(\ell)}\right)^* = \frac{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell))}) u_{k,i}^{(\ell)} \tau_{ic}}{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell))}) \tau_{ic}}
    $$

    Similarly, the optimization objective of $\alpha_c$ is close to the one of proposition \ref{MLE_abundance_bernoulli_tree},
    up to a multiplication by $\tau_{ic}$.
    Therefore, we refer to \ref{MLE_abundance_bernoulli_tree} for the original computation, that just requires a multiplication by
    $\tau_{ic}$ to obtain the following result:
    $$
    \left[\alpha_{c,k}^{(\ell)}\right]_v &\leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[\alpha_{c,k}^{(\ell)}\right]_u\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]\tau_{ic}}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \tau_{ic}} \right) \right)
    $$

\end{proof}