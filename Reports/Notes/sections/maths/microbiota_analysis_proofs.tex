\section{Appendix}

\subsection{Microbiota analysis}

\begin{proposition}(Bernoulli tree prior)
    \label{proposition:bernoulli_tree_prior}
    \\
    Let $T = (T^{(1)}, \dots, T^{(L)})$ a random tree of depth $L$. \\
    For all $\ell \in \{1, \dots, L\}$, denote $T^{(\ell)} = (u_1^{(\ell)}, \dots, u_{K_{\ell}}^{(\ell)})$ the nodes at layer $\ell$,
    such that $u_k^{(\ell)} \in \{0, 1\}$ denotes whether or not the node is activated (1 if activated). \\
    Denote $\mathcal{P}(u_k^{(\ell)})$ the parent of the node $u_k^{(\ell)}$ that outputs $1$ if the parent exists and is activated, $0$ otherwise. \\
    Assume that:
    \begin{itemize}
        \item $p(T^{(1)}) = \delta_{e_1}(T^{(1)})$
        \item $\forall l \geq 2, p(T^{(\ell+1)} | T^{(1:\ell)}) = p(T^{(\ell+1)} | T^{(\ell)})$
        \item $\forall l \geq 2, k \in \{1, \dots, K_{\ell}\}, u_k^{(\ell)} | \left\{\mathcal{P}(u_k^{(\ell)}) = 1 \right\} \sim \mathcal{B}(\pi_k^{(\ell)}) \right$
        \item $\displaystyle p\left(u_1^{(\ell)}, \dots, u_{K_{\ell}}^{(\ell)} | \mathcal{P}(u_1^{(\ell)}), \mathcal{P}(u_{K_{\ell}}^{(\ell)})\right) = \prod_{k=1}^{K_{\ell}} p\left(u_k^{(\ell)} | \mathcal{P}(u_k^{(\ell)})\right)$
    \end{itemize}

    Then, the distribution of $T$ can be written as:
    $$
    p(T) = \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left(\left(\pi_k^{(\ell+1)}\right)^{u_k^{(\ell+1)}} \left(1-\pi_k^{(\ell+1)}\right)^{1 - u_k^{(\ell+1)}} \right)^{\mathcal{P}(u_k^{(\ell+1)})}
    $$
\end{proposition}

\begin{proof}
    $$
    \begin{align}
        p(T) &= p\left(T^{(1)}, \dots, T^{(L)}\right) \\
        &= p\left(T^{(1)}\right) \prod_{l=1}^{L-1} p\left(T^{(\ell+1)}|T^{(\ell)}\right) \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} p\left(u_1^{(\ell+1)}, \dots, u_{K_{\ell}}^{(\ell+1)} | \nodeparent(u_1^{(\ell+1)}), \dots, \nodeparent(u_{K_{\ell}}^{(\ell+1)})\right) \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} p\left(u_k^{(\ell+1)} | \nodeparent(u_k^{(\ell+1)}) \right) \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left( \pi_k^{(\ell+1)} u_k^{(\ell+1)} + \left(1 - \pi_k^{(\ell+1)} \right)(1 - u_k^{(\ell+1)}) \right)^{\nodeparent(u_k^{(\ell+1)})} \\
        &= \delta_{e_1}(T^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left( \left(\pi_k^{(\ell+1)}\right)^{u_k^{(\ell+1)}} \left(1 - \pi_k^{(\ell+1)} \right)^{1 - u_k^{(\ell+1)}} \right)^{\nodeparent(u_k^{(\ell+1)})}
    \end{align}
    $$
\end{proof}

\begin{proposition}[MLE of the Bernoulli tree prior]
    \label{proposition:mle_bernoulli_tree_prior}
    \\
    Recall the context of \ref{proposition:bernoulli_tree_prior}. \\
    Let $(T_1, \dots, T_n)$ $n$ trees i.i.d following the bernoulli tree prior.
    Denote the maximum likelihood objective by
    $$
    \begin{equation}
        \begin{align}
            arg \max_{\pi_j^{(m)}} \quad & \sum_{i=1}^n \sum_{l=1}^{L-1} \sum_{k=1}^{K_{\ell}} \nodeparent(u_{k,i}^{(\ell+1)}) \left[u_{k,i}^{(\ell+1)} \log \pi_{k}^{(\ell+1)} + (1 - u_{k,i}^{(\ell+1)}) \log (1 - \pi_k^{(\ell+1)}) \right] \\
            \textrm{s.t.} \quad & \forall k,l, \pi_{k}^{(\ell)} \in [0, 1] \\
        \end{align}
        \label{eq:prior_transition_objective}
    \end{equation}
    $$

    Then the maximum likelihood estimator is given by
    $$
    \left(\pi_k^{(\ell)}\right)^* = \frac{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)}) u_{k,i}^{(\ell)}}{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)})}
    $$
\end{proposition}

\begin{proof}
    Simply by deriving the objective we obtain:
    $$
    \begin{align}
        \partial_{\pi_j^{(m)}} \log p(T_1, \dots, T_n) &= \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) \left[ u_{j,i}^{(m)} \frac{1}{\pi_j^{(m)}} + (1 - u_{j,i}^{(m)}) \frac{-1}{1-\pi_j^{(m)}}\right] \\
        &= \frac{1}{\pi_j^{(m)}} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)} - \frac{1}{1 - \pi_j^{(m)}} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) (1 - u_{j,i}^{(m)})
    \end{align}
    $$

    Looking for $0$ valued gradient, we end up with:
    $$
    \begin{align}
    (1 - \pi_j^{(m)}) \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)} - \pi_j^{(m)} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) (1 - u_{j,i}^{(m)}) &= 0 \\
    \pi_j^{(m)} \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) &= \sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)} \\
    \pi_j^{(m)} &= \frac{\sum_{i=1}^n \nodeparent(u_{j,i}^{(m)}) u_{j,i}^{(m)}}{\sum_{i=1}^n \nodeparent(u_{j,i}^{(m)})}
    \end{align}
    $$

    Since the obtained value respects the constraint, that concludes the proof.
\end{proof}

\begin{proposition}[Law of $aX$]
    \label{proposition:law_of_aX}
    \\
    Let $X \sim f(x)$ a random variable with density $f$. \\
    Let $a \in (0, 1)$. \\
    The distribution of $aX$ is given by:
    $$p_{aX}(u) = \frac{1}{a} f\left(\frac{u}{a}\right)$$
\end{proposition}

\begin{proof}
    We use the transfer theorem.
    Let $g$ a continuous and measurable function.
    $$
    \begin{align}
        \mathbb{E}[g(aX)] &= \int g(ax) f(x) dx \\
                          &= \int g(u) f\left(\frac{u}{a}\right) \frac{1}{a} du
    \end{align}
    $$
    Hence, one can identify the distribution of $aX$ as $p_{aX}(u) = \frac{1}{a} f\left(\frac{u}{a}\right)$
\end{proof}

\begin{proposition}[Abundance distribution conditionally to the trees]
    \label{proposition:abundance_posterior_bernoulli_tree}

    \newline

    Let $X = (X^{(1)}, \dots, X^{(L)})$ the abundance matrix of a tree $T$ with depth $L$. \\
    For all $\ell \in \{1, \dots, L\}$, denote $X^{(\ell)} = (x_1^{(\ell)}, \dots, x_{K_{\ell}}^{(\ell)})$ the abundance value of each node of the tree. \\
    Denote $\childrennode(x_k^{(\ell)})$ the vector of children abundances related to $x_k^{(\ell)}$, which is empty if it has no children. \\
    Denote $T^{(\ell + 1)}_k$ the vector of activation states $(u_j^{(\ell+1)})_j$ of the children of the node $u_k^{(l)}$ in $T$. \\
    Assume that:
    \begin{itemize}
        \item $p(X^{(1)}) = \delta_{e_1}(X^{(1)})$
        \item $p(X^{(\ell+1)} | X^{(1:\ell)}, T) = p(X^{(\ell+1)} | X^{(\ell)}, T^{(\ell+1)})$
        \item $p(X^{(\ell+1)} | X^{(\ell)}, T^{(\ell+1)}) = \displaystyle\prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, T^{(\ell+1)})$
        \item For all $l \in \{2, \dots, L\}, k \in \{1, \dots, K_{\ell}\}$,
              \begin{itemize}
                  \item If $|\childrennode(x_k^{(\ell)})| > 1$: $\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, T^{(\ell+1)} \sim x_k^{(\ell)} D(\alpha_k^{(\ell)} \odot T^{(\ell+1)}_k)$
                  \item If $|\childrennode(x_k^{(\ell)})| = 1$, $\childrennode(x_k^{(\ell)}) |  x_k^{(\ell)}, T^{(\ell+1)} \sim \delta_{x_k^{(\ell)}}\left(\childrennode(x_k^{(\ell)})\right)$
              \end{itemize}
    \end{itemize}

    Then, the distribution of $X$ conditionally to $T$ is given by:
    $$
        p(X|T) = \delta_{e_1}(X^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left(
    \delta_{x_k^{(\ell)}}\left(\childrennode(x_k^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| = 1}}
    \frac{1}{x_k^{(\ell)}} f_{\alpha_k^{(\ell)} \odot T^{(\ell+1)}_k} \left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right)^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| > 1}}
                \right)
    $$
    
\end{proposition}

\begin{proof}
    $$
    \begin{align}
        p(X | T) &= p(X^{(1)}, \dots, X^{(L)} | T) \\
                &= p(X^{(1)} | T^{(1)}) \prod_{l=1}^{L-1} p(X^{(\ell+1)} | X^{(\ell)}, T^{({\ell}+1)}) \\
                &= \delta_{e_1}(X^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, T^{(\ell+1)}) \\
                &= \delta_{e_1}(X^{(1)}) \prod_{l=1}^{L-1} \prod_{k=1}^{K_{\ell}} \left(
                                                                                \delta_{x_k^{(\ell)}}\left(\childrennode(x_k^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| = 1}}
                                                                                \left[\underbrace{\frac{1}{x_k^{(\ell)}} f_{\alpha_k^{(\ell)} \odot T^{(\ell+1)}_k} \left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right)}_{\text{proposition \ref{proposition:law_of_aX}}}\right]^{\mathds{1}_{|\childrennode(x_k^{(\ell)})| > 1}}
                                                                            \right)
    \end{align}
    $$
\end{proof}

\begin{proposition}[MLE of the abundance distribution a posteriori]
    \label{proposition:MLE_abundance_bernoulli_tree}
    \\
    Consider the context of proposition \ref{proposition:abundance_posterior_bernoulli_tree}. \\
    We consider a data set $(X_i, T_i)_{1 \geq i \geq n}$ of abundance matrix and trees. \\
    Denote by $\mathcal{V}(\alpha_k^{(\ell)}, T) = \left\{v \in \mathbb{N} | \left[ \alpha_k^{(\ell)} \odot T^{(\ell+1)}_k \right]_v \neq 0\right\}$
    the set of indexes so that the coordinate of $\alpha_k^{(\ell)}$ is not masked by $T^{(\ell+1)}$. \\
    The maximum likelihood estimator of the distribution characterising the abundance conditionally to the tree is then
    given by the following fixed point algorithm: \\

    $
    \forall l \in \{1, \dots, L\}, k \in \{1, \dots, K_{\ell}\}, v \in \{1, \dots, |\alpha_k^{(\ell)}|\}
    $,

    $$
    \alpha_{k,v}^{(\ell)} \leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)}\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)}} \right) \right)
    $$
\end{proposition}

\begin{proof}
    The maximum likelihood objective can be written as:
    $$
    \begin{align*}
        arg \max_{\alpha_{j,v}^{(m)}} \quad & \sum_{i=1}^n \sum_{l=1}^{L-1} \sum_{k=1}^{K_{\ell}} \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \log f_{\alpha_k^{(\ell)} \odot T_{k,i}^{(\ell+1)}} \left(\frac{C(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) \\
    \end{align*}
    $$
    Using the Dirichlet distribution expression, we can write the following:
    \small
    $$
    \begin{align}
        \log f_{\alpha_k^{(\ell)} \odot T_{k,i}^{(\ell+1)}}\left(\frac{C(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) = \log \Gamma \left(\sum_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,v}^{(\ell)}\right)
        - \sum_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \Gamma \left( \alpha_{k,v}^{(\ell)} \right)
        + \sum_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left(\alpha_{k,v}^{(\ell)} - 1\right) \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}}
    \end{align}
    $$
    \normalsize

    Writing $\psi$ the digamma function, the derivative of the objective relatively to a fixed $\alpha_{k,v}^{(\ell)}$ is given by:
    $$
    \partial_{\alpha_{k,v}^{(\ell)}} \log p(X|T) = \sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[\psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)} \right) - \psi\left(\alpha_{k,v}^{(\ell)} \right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}}\right]
    $$

    Looking for $0$ valued gradient, we obtain the following equation:
    $$
    \psi\left(\alpha_{k,v}^{(\ell)}\right) = \frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)}\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]}
                                                {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)}} \right)
    $$

    Using the reference trick from \cite{dirichlet_digamma_trick}, one can compute an iterative fixed point algorithm to solve the previous equation,
    leading to:
    $$
    \alpha_{k,v}^{(\ell)} \leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)} \alpha_{k,u}^{(\ell)}\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_k^{(\ell)}, T_i)}} \right) \right)
    $$
\end{proof}


\begin{proposition}[Expectation-Maximization for Taxonomy Clustering model]
    \label{proposition:EM_taxonomy_clustering}
    \newline
    We use the same context as for proposition \ref{proposition:abundance_posterior_bernoulli_tree} of this section.
    Assume the following framework changes:
    \begin{itemize}
        \item $(X_i, T_i)_{1 \le i \le n}$ samples of taxa-abundance data ($X_i$ abundance, $T_i$ associated taxonomy)
        \item Let $Z_i$ a latent variable taking values in $\{1, \dots, C\}$.
        \item We model the distributions as follow:
            $$
            \begin{align}
                p(Z_i = c) &= \gamma_c \\
                p\left(u_{k,i}^{(\ell)} = 1 | \nodeparent(u_{k,i}^{(\ell)}) = 1, Z_i = c\right) &= \pi_{c,k}^{(\ell)} \\
                p\left(\childrennode(x_{k,i}^{(\ell)}) | x_{k,i}^{(\ell)}, T^{(\ell+1)}, Z_i = c \right) &\sim x_k^{(\ell)} \mathcal{D}(\alpha_{c,k}^{(\ell)} \odot T_k^{(\ell+1)}) \quad \text{if } |\childrennode(x_k^{(\ell)})| > 1
            \end{align}
            $$
    \end{itemize}

    Consider the optimization problem:
    $$
    \theta^* = arg\max_{\theta} \mathbb{E}_{p_{\widehat{\theta}}}[\log p_{\theta}(X, T, Z) | X, T]
    $$

    Then, the Expectation-Maximization algorithm solving that problem is given by (for each sample $i$ and cluster index $c$): \\

    \textbf{Expectation step}:

        $$
        p_{\widehat{\theta}}(Z_i = c | X_i, T_i) = \tau_{ic} =
                        \frac{\widehat{\gamma}_c p_{\widehat{\theta}}(T_i | Z_i = c) p_{\widehat{\theta}}(X_i | T_i, Z_i = c)
                             }
                             {
                                \sum_{c'=1}^C
                                \widehat{\gamma}_{c'} p_{\widehat{\theta}}(T_i | Z_i = c') p_{\widehat{\theta}}(X_i | T_i, Z_i = c')
                             }
        $$

        where:
        $$
        \begin{align}
            &p_{\widehat{\theta}}(T_i | Z_i = c) = \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \left[\left(\widehat{\pi}_{c,k}^{(\ell+1)}\right)^{u_{k,i}^{(\ell+1)}} \left(1 - \widehat{\pi}_{c,k}^{(\ell+1)}\right)^{1 - u_{k,i}^{(\ell + 1)}}\right]^{\nodeparent(u_{k,i}^{(\ell+1)})} \\
            &p_{\widehat{\theta}}(X_i | T_i, Z_i = c) = \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \delta_{x_{k,i}^{(\ell)}}\left(\childrennode(x_{k,i}^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| = 1}} \left(\frac{1}{x_{k,i}^{(\ell)}} f_{\widehat{\alpha}_{c,k}^{(\ell)} \odot T_{i,k}^{(\ell+1)}}\left(\frac{\childrennode(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) \right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1}}
        \end{align}
        $$

    \medskip
    \textbf{Maximization step}:

    \medskip

    $\displaystyle
    \gamma_c^*  = \frac{1}{n} \sum_{i=1}^n \tau_{ic}
    $

    $\displaystyle
        \left(\pi_{c,k}^{(\ell)}\right)^* &= \frac{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)}) u_{k,i}^{(\ell)} \tau_{ic}}{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell)}) \tau_{ic}} \\
    $

    $\displaystyle
    \left[\alpha_{c,k}^{(\ell)}\right]_v &\leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[\alpha_{c,k}^{(\ell)}\right]_u\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]\tau_{ic}}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \tau_{ic}} \right) \right)
    $
\end{proposition}

\begin{proof}

    We study the two parts of the EM algorithm, starting with the expectation step.

    \medskip

    \textbf{Expectation step}:

    \medskip

    Applying Bayes formula, we obtain:
    $$
    \begin{align}
        p_{\widehat{\theta}}(Z_i = c | X_i, T_i) &= \tau_{ic} \\
                              &= \frac{p_{\widehat{\theta}}(Z_i = c, X_i, T_i)}{\sum_{c'=1}^C p_{\widehat{\theta}}(Z_i = c', X_i, T_i)} \\
                              &= \frac{\widehat{\gamma}_c p_{\widehat{\theta}}(T_i | Z_i = c) p_{\widehat{\theta}}(X_i | T_i, Z_i = c)}{\sum_{c'=1}^C \widehat{\gamma}_{c'} p_{\widehat{\theta}}(T_i = Z_i = c') p_{\widehat{\theta}}(X_i | T_i, Z_i = c')}
    \end{align}
    $$

    Exploiting the model framework, we obtain the following expression for the conditional distribution of the taxonomy:
    $$
    \begin{align}
        p_{\theta}(T_i | Z_i = c) &= p_{\theta}(T_i^{(1)}, \dots, T_i^{(L)} | Z_i = c) \\
                        &= \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} p_{\theta}(T_i^{(\ell + 1)} | T_i^{(\ell)}, Z_i = c) \\
                        &= \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} p_{\theta}(u_{k,i}^{(\ell+1)} | \nodeparent(u_{k,i}^{(\ell+1)}), Z_i = c) \\
                        &= \delta_{e_1}(T_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \left[\left(\pi_{c,k}^{(\ell+1)}\right)^{u_{k,i}^{(\ell+1)}} \left(1 - \pi_{c,k}^{(\ell+1)}\right)^{1 - u_{k,i}^{(\ell + 1)}}\right]^{\nodeparent(u_{k,i}^{(\ell+1)})}
    \end{align}
    $$

    Similarly, the conditional abundance distribution is given by:

    $$
    \begin{align}
        p_{\theta}(X_i | T_i, Z_i = c) &= p_{\theta}(X_i^{(1)}, \dots, X_i^{(L)} | T_i, Z_i = c) \\
                                        &= \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} p_{\theta}(X_i^{(\ell+1)} | X_i^{(\ell)}, T_i^{(\ell+1)}, Z_i = c) \\
                                        &= \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} p_{\theta}(\childrennode(x_{k,i}^{(\ell)}) | x_{k,i}^{(\ell)}, T_i^{(\ell+1)}, Z_i=c) \\
                                        &= \delta_{e_1}(X_i^{(1)}) \prod_{\ell = 1}^{L-1} \prod_{k=1}^{K_{\ell}} \delta_{x_{k,i}^{(\ell)}}\left(\childrennode(x_{k,i}^{(\ell)})\right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| = 1}} \left(\frac{1}{x_{k,i}^{(\ell)}} f_{\alpha_{c,k}^{(\ell)} \odot T_{i,k}^{(\ell+1)}}\left(\frac{\childrennode(x_{k,i}^{(\ell)})}{x_{k,i}^{(\ell)}} \right) \right)^{\mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1}}\\
    \end{align}
    $$

    \medskip

    \textbf{Maximization step}

    \medskip

    Recall the general objective:
    $$
    \begin{align}
        \theta^* &= arg\max_{\theta} Q(\widehat{\theta}, \theta) \\
                 &= arg\max_{\theta} \frac{1}{n} \sum_{i=1}^n \sum_{c=1}^C \left(\log \gamma_c + \log p(T_i | Z_i = c) + \log p(X_i | T_i, Z_i = c)\right) \tau_{ic}
    \end{align}
    $$

    We start by optimizing $\gamma$.
    The objective can be written as:
    $$
    \begin{equation}
        \begin{align}
            arg \max_{\gamma_j} \quad & \sum_{i=1}^n \sum_{c=1}^{C} \tau_{ic} \log \gamma_c  \\
            \textrm{s.t.} \quad & \sum_{c=1}^{C} \gamma_c = 1 \\
                                & \forall c, \gamma_c \geq 0
        \end{align}
        \label{eq:objective_latent_gamma_c_microbiota_clustering}
    \end{equation}
    $$

    This optimization problem can be solved using Lagrange duality.
    KKT conditions are then written as (optimizing relatively to any $\gamma_c$):
    $$
    \begin{align}
        &\frac{1}{\gamma_c} \sum_{i=1}^n \tau_{ic} + \phi - \psi = 0 \\
        &\sum_{c=1}^C \gamma_c = 1 \\
        &\psi \geq 0 \\
        &\forall c, \gamma_c \geq 0 \\
        &\psi \gamma_c = 0
    \end{align}
    $$

    Using the first identity, we get:
    $$
    \gamma_c = \frac{1}{\psi - \phi} \sum_{i=1}^n \tau_{ic}
    $$

    Let's set $\phi = 0$, which enables use to ensure the last identity.
    Using the second identity, we then have:
    $$
    \begin{align}
        \sum_{c=1}^C \gamma_c = 1 &= \sum_{c=1}^C \frac{1}{\psi} \sum_{i=1}^n \tau_{ic} \\
                                    &= \frac{1}{\psi} \sum_{i=1}^n \sum_{c=1}^C \tau_{ic} \\
                                    &= \frac{1}{\psi} \sum_{i=1}^n 1 \\
                                    &= \frac{n}{\psi}
    \end{align}
    $$

    Hence, we get $\psi = n$, which leads us to the optimal $\gamma_c$ that verifies the KKT constraints:
    $$
    \gamma_c^* = \frac{1}{n} \sum_{i=1}^n \tau_{ic}
    $$

    Looking at $\pi_c$ now, the optimization objective can be written as:
    $$
    \begin{equation}
        \begin{align}
            arg \max_{\pi_{c,k}^{(\ell)}} \quad & \sum_{i=1}^n \sum_{c=1}^{C} \tau_{ic} \log p_{\pi_c}(T_i | Z_i = c)  \\
            \textrm{s.t.} \quad & \pi_{c,k}^{(\ell)} \in [0, 1]
        \end{align}
    \end{equation}
    $$

    This objective is very similar to the MLE computed for proposition \ref{proposition:mle_bernoulli_tree_prior}, up to
    a multiplication by $\tau_{ic}$.
    Hence, we refer to the proposition \ref{proposition:mle_bernoulli_tree_prior} for the computation, which leads to the following
    optimal $\pi_c$:
    $$
    \left(\pi_{c,k}^{(\ell)}\right)^* = \frac{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell))}) u_{k,i}^{(\ell)} \tau_{ic}}{\sum_{i=1}^n \nodeparent(u_{k,i}^{(\ell))}) \tau_{ic}}
    $$

    Similarly, the optimization objective of $\alpha_c$ is close to the one of proposition \ref{MLE_abundance_bernoulli_tree},
    up to a multiplication by $\tau_{ic}$.
    Therefore, we refer to \ref{MLE_abundance_bernoulli_tree} for the original computation, that just requires a multiplication by
    $\tau_{ic}$ to obtain the following result:
    $$
    \left[\alpha_{c,k}^{(\ell)}\right]_v &\leftarrow \psi^{-1} \left(\frac{\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[ \psi\left(\sum_{u \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \left[\alpha_{c,k}^{(\ell)}\right]_u\right) + \log \frac{\left[\childrennode(x_{k,i}^{(\ell)})\right]_v}{x_{k,i}^{(\ell)}} \right]\tau_{ic}}
    {\sum_{i=1}^n \mathds{1}_{|\childrennode(x_{k,i}^{(\ell)})| > 1} \mathds{1}_{v \in \mathcal{V}(\alpha_{c,k}^{(\ell)}, T_i)} \tau_{ic}} \right) \right)
    $$

\end{proof}

\begin{proposition}[Likelihood of layer-wise hidden markov model tree generator]
    \label{proposition:likelihood_layer_wise_hmm_tree}
    Let us assume the following framework:
    \begin{itemize}
        \item $Z$ follows a discrete Markov Chain, such that for all $c \in \{1, \dots, C\}$:
        $$
        \begin{align}
            &\mathbb{P}(Z^{(1)} = c) = a_c \\
            &\mathbb{P}(Z^{(\ell+1)} = k | Z^{(\ell)} = c) = t^{(\ell+1)}_{c,k}
        \end{align}
        $$

        \item $T$ denotes a tree, for which the layers are denoted by $(T^{(\ell)})_{1 \leq \ell \leq L}$
                and the internal nodes by $u_k^{(\ell)}$ (Bernoulli variables).

        \item We introduce $\childrennode(T^{(\ell)})$ the set of plausible children in the
                taxonomy at layer $(\ell+1)$, conditionally to $T^{(\ell)}$.
        \item The emission probability is then modeled by:
            $$\mathbb{P}(T^{(\ell+1)} = \omega | Z^{(\ell+1)} = c, T^{(\ell)}) = \frac{\pi_c(\omega) \mathds{1}_{\omega \in \childrennode(T^{(\ell)})}}{\sum_{\nu \in \childrennode(T^{(\ell)})} \pi_c(\omega)}$$
    \end{itemize}

    Then, the joint likelihood of such model is given by:
    $$
    p(T, Z) = p(Z^{(1)}) \delta_{e_1}(T^{(1)}) \prod_{\ell=1}^{L-1} p(T^{(\ell+1)} | Z^{(\ell+1)}, T^{(\ell)}) p(Z^{(\ell+1)} | Z^{(\ell)})
    $$

\end{proposition}

\begin{proof}

    $$
    \begin{align}
        p(T, Z) &= p(T^{(1)}, Z^{(1)}, \dots, T^{(L)}, Z^{(L)}) \\
                &= p(T^{(1)}, Z^{(1)}) \prod_{\ell = 1}^{L-1} p(T^{(\ell+1)}, Z^{(\ell+1)} | T^{(\ell)}, Z^{(\ell)}) \\
                &= p(Z^{(1)}) p(T^{(1)} | Z^{(1)}) \prod_{\ell=1}^{L-1} p(T^{(\ell+1)} | Z^{(\ell+1)}, Z^{(\ell)}, T^{(\ell)}) p(Z^{(\ell+1)} | Z^{(\ell)}, T^{(\ell)}) \\
                &= p(Z^{(1)}) \delta_{e_1}(T^{(1)}) \prod_{\ell=1}^{L-1} p(T^{(\ell+1)} | Z^{(\ell+1)}, T^{(\ell)}) p(Z^{(\ell+1)} | Z^{(\ell)})
    \end{align}
    $$

\end{proof}

\begin{proposition}[Expectation-Maximization for layer-wise hidden markov model for tree generation]
    \label{proposition:em_layer_wise_hmm_tree}
    Consider the framework of proposition \ref{proposition:likelihood_layer_wise_hmm_tree}.
    \medskip
    Recall the Expectation-Maximization objective:
    $$
    \theta^* = arg\max_{\theta} Q(\widehat{\theta}, \theta)
    $$
    where $Q(\widehat{\theta}, \theta) = \mathbb{E}_{p_{\widehat{\theta}}(Z|T)}[\log p(T,Z) | T]$.
    \medskip
    The E-M algorithm is then given by:
    \begin{itemize}
        \item \textbf{Expectation step}:

            \medskip

            Let $\nu = (c_1, \dots, c_L)$.
            We define $\tau_{i\nu} = p(Z_i = \nu | T_i)$. Then we have:
            \footnotesize
            $$
            \begin{align}
                \tau_{i\nu} = \dfrac{
                    p_{\widehat{\theta}}(Z_i^{(1)} = c_1) \delta_{e_1}(T_i^{(1)}) \prod_{\ell=1}^{L-1} p_{\widehat{\theta}}(T_i^{(\ell+1)} | Z_i^{(\ell+1)} = c_{\ell+1}, T_i^{(\ell)}) p_{\widehat{\theta}}(Z_i^{(\ell+1)} = c_{\ell+1} | Z_i^{(\ell)} = c_{\ell})
                }
                {
                    \sum_{c_1'=1}^C \dots \sum_{c_L'=1}^C p_{\widehat{\theta}}(Z_i^{(1)} = c_1') \delta_{e_1}(T^{(1)}) \prod_{\ell=1}^{L-1} p_{\widehat{\theta}}(T_i^{(\ell+1)} | Z_i^{(\ell+1)} = c_{\ell+1}', T_i^{(\ell)}) p_{\widehat{\theta}}(Z_i^{(\ell+1)} = c_{\ell+1}' | Z_i^{(\ell)} = c_{\ell}')
                }
            \end{align}
            $$
            \normalsize
        \item \textbf{Maximization step}:

            \medskip

            The prior on the markov chain root can be maximized explicitly:
            $$
            a_c^* = \frac{1}{n} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \mathds{1}_{c_1 = c} \tau_{i\nu}
            $$

            The emission probabilities can be computed through an iterative proximal gradient ascent algorithm.

            \medskip

            The transition probabilities can be maximized explicitly:
            $$
            t_{c,k}^{(\ell)}^* = \frac{1}{\sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \mathds{1}_{c_{\ell-1}=c}} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \mathds{1}_{c_{\ell-1}=c} \mathds{1}_{c_{\ell}=k}
            $$
    \end{itemize}
\end{proposition}

\begin{proof}
    We start by showing the \textbf{Expectation step}:
    \scriptsize
    $$
    \begin{align}
        \tau_{i\nu} &= p_{\widehat{\theta}}(Z_i^{(1)} = c_1, \dots, Z_i^{(L)} = c_L | T_i) \\
                    &= \dfrac{p_{\widehat{\theta}}(Z_i^{(1)} = c_1, \dots, Z_i^{(L)} = c_L, T_i) }{\sum_{c_1'=1}^{C} \dots \sum_{c_L' = L}^{C} p_{\widehat{\theta}}(Z_i^{(1)} = c_1', \dots, Z_i^{(L)} = c_L', T_i)} \\
                    &= \dfrac{p_{\widehat{\theta}}(Z_i^{(1)} = c_1) p_{\widehat{\theta}}(T_i^{(1)} | Z_i^{(1)} = c_1) \prod_{\ell=1}^{L-1} p_{\widehat{\theta}}(T_i^{(\ell+1)} | Z_i^{(\ell+1)} = c_{\ell+1}, T_i^{(\ell)}, Z_i^{(\ell)} = c_{\ell}) p_{\widehat{\theta}}(Z_i^{(\ell+1)} = c_{\ell+1} | Z_i^{(\ell)} = c_{\ell}, T_i^{(\ell)})}
                        {\sum_{c_1'=1}^C \dots \sum_{c_L' = 1}^C p_{\widehat{\theta}}(Z_i^{(1)} = c_1') p_{\widehat{\theta}}(T_i^{(1)} | Z_i^{(1)} = c_1') \prod_{\ell=1}^{L-1} p_{\widehat{\theta}}(T_i^{(\ell+1)} | Z_i^{(\ell+1)} = c_{\ell+1}', T_i^{(\ell)}, Z_i^{(\ell)} = c_{\ell}') p_{\widehat{\theta}}(Z_i^{(\ell+1)} = c_{\ell+1}' | Z_i^{(\ell)} = c_{\ell}', T_i^{(\ell)})} \\
                    &= \dfrac{
                            p_{\widehat{\theta}}(Z_i^{(1)} = c_1) \delta_{e_1}(T_i^{(1)}) \prod_{\ell=1}^{L-1} p_{\widehat{\theta}}(T_i^{(\ell+1)} | Z_i^{(\ell+1)} = c_{\ell+1}, T_i^{(\ell)}) p_{\widehat{\theta}}(Z_i^{(\ell+1)} = c_{\ell+1} | Z_i^{(\ell)} = c_{\ell})
                        }
                        {
                            \sum_{c_1'=1}^C \dots \sum_{c_L'=1}^C p_{\widehat{\theta}}(Z_i^{(1)} = c_1') \delta_{e_1}(T^{(1)}) \prod_{\ell=1}^{L-1} p_{\widehat{\theta}}(T_i^{(\ell+1)} | Z_i^{(\ell+1)} = c_{\ell+1}', T_i^{(\ell)}) p_{\widehat{\theta}}(Z_i^{(\ell+1)} = c_{\ell+1}' | Z_i^{(\ell)} = c_{\ell}')
                        }
    \end{align}
    $$
    \normalsize

    We now prove the \textbf{Maximization step}. Recall the maximization objective:
    $$
    \theta = arg\max_{\theta} Q(\widehat{\theta}, \theta)
    $$

    We use the notation $\nu = (c_1, \dots, c_L)$.
    Using the likelihood computed at proposition \ref{proposition:likelihood_layer_wise_hmm_tree}, the maximization term becomes:
    \footnotesize
    $$
    \begin{align}
        Q(\widehat{\theta}, \theta) &= \mathbb{E}_{p_{\widehat{\theta}}(Z | T)}\left[\log p_{\theta}(T, Z) | T\right] \\
                                    &= \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \left[\log a_{c_1} + \sum_{\ell=1}^{L-1} \mathds{1}_{T_i^{(\ell+1)} \in \childrennode(T_i^{(\ell)})} \left(\log \pi_{c_{\ell+1}}^{(\ell+1)}(T_i^{(\ell+1)}) - \log \sum_{\mu \in \childrennode(T_i^{(\ell)})} \pi_{c_{\ell+1}}^{(\ell+1)}(\mu) + \log t_{c_{\ell}, c_{\ell+1}}^{(\ell+1)}\right) \right] \tau_{i\nu}
    \end{align}
    $$
    \normalsize

    We obtain an explicit formula to compute the optimal parameters of the markov chain and emission probability:
    \begin{itemize}
        \item The optimization regarding $a = (a_{1}, \dots, a_{C})$ is given by the following objective:
            $$
            \begin{align}
                a_c^* = &arg\max_{a_c} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \log a_{c_1} \\
                        &\text{s.t.} \quad \sum_{c=1}^C a_c = 1 \\
                        &\quad \quad \forall c, a_c > 0
            \end{align}
            $$

            This optimization problem is solvable through Lagrange duality, leading to the following optimal result:
            $$
            a_c^* = \frac{1}{n} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \mathds{1}_{c_1 = c} \tau_{i\nu}
            $$

        \item For $\omega \in \mathcal{T}(\ell)$, where $\mathcal{T}(\ell)$ denotes the possible activation vectors in the global taxonomy $\mathcal{T}$,
            the optimization regarding the emission states parameters $\pi_c^{(\ell)}(\omega)$ is given by:
            $$
            \begin{align}
                \pi_c^{(\ell)}(\omega)^* = &arg\max_{\pi_c^{(\ell)}(\omega)} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \sum_{\ell=1}^{L-1} \mathds{1}_{T_i^{(\ell+1)} \in \childrennode(T_i^{(\ell)})} \left(\log \pi_{c_{\ell+1}}^{(\ell+1)}(T_i^{(\ell+1)}) - \log \sum_{\mu \in \childrennode(T_i^{(\ell)})} \pi_{c_{\ell+1}}^{(\ell+1)}(\mu)\right) \\
                &\text{s.t.} \quad \sum_{\nu \in \mathcal{T}(\ell)} \pi_c^{(\ell)}(\nu) = 1 \\
                &\quad \quad \forall \nu \in \mathcal{T}(\ell), \pi_c^{(\ell)}(\nu) > 0
            \end{align}
            $$

            The derivative relatively to $\pi_c^{(\ell)}(\omega)$ is then given by:
            $$
            \frac{\partial Q(\widehat{\theta}, \theta)}{\partial \pi_{c}^{(\ell)}(\omega)} = \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \mathds{1}_{\omega \in \childrennode(T_i^{(\ell-1)})} \mathds{1}_{c_{\ell} = c} \left(\frac{1}{\pi_c^{(\ell)}(\omega)} - \frac{1}{\sum_{\mu \in \childrennode(T_i^{(\ell-1)})} \pi_c^{(\ell)}(\mu)}\right)
            $$

            Since the objective is convex, this leads to a fixed point algorithm to compute the emission states parameters like proximal gradient descent to keep the constraint.

        \item The optimization objective regarding the transition probabilities of the markov chain $t_{c,k}^{(\ell)}$ is given by:
            $$
            \begin{align}
                t_{c,k}^{(\ell)}^* = &arg\max_{t_{c,k}^{(\ell)}} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \sum_{\ell=1}^{L-1} \mathds{1}_{T_i^{(\ell+1)} \in \childrennode(T_i^{(\ell)})} \log t_{c_{\ell},c_{\ell+1}}^{(\ell+1)} \\
                &\text{s.t.} \quad \sum_{j = 1}^C t_{c,j}^{(\ell)} = 1 \\
                &\quad \quad \forall j, t_{c,j}^{(\ell)} > 0
            \end{align}
            $$

            Using Lagrange duality, we can solve the optimization problem to obtain the following optimal parameter:
            $$
            t_{c,k}^{(\ell)}^* = \frac{1}{\sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \mathds{1}_{c_{\ell-1}=c}} \sum_{i=1}^n \sum_{\nu \in \mathcal{S}(Z)} \tau_{i\nu} \mathds{1}_{c_{\ell-1}=c} \mathds{1}_{c_{\ell}=k}
            $$
    \end{itemize}

\end{proof}

\begin{proposition}[Joint distribution of Bottom-Up Hidden Tree Markov Model applied to top-down markov tree data]
    \label{proposition:likelihood_bottom_up_htmm_taxaabundance_data}
    Let $X$ a top-down tree markov model.
    Let $Z$ a Hidden Tree Markov Model defined over $X$.

    Then the joint likelihood is given by:
    $$
    p(X, Z) = p(X^{(1)}) p(Z^{(\ell)}) \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) p(Z_k^{(\ell)} | \childrennode(Z_k^{(\ell)}))
    $$
\end{proposition}

\begin{proof}
    $$
    \begin{align}
        p(X, Z) &= p(X^{(1)}, Z^{(1)}, \dots, X^{(L)}, Z^{(L)}) \\
                &= p(X^{(1)}) p(Z^{(L)}) \prod_{\ell=1}^{L-1} p(X^{(\ell+1)}, Z^{(\ell)} | X^{(\ell)}, Z^{(\ell+1)}) \\
                &= p(X^{(1)}) p(Z^{(L)}) \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}), Z_k^{(\ell)} | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) \\
                &= p(X^{(1)}) p(Z^{(L)}) \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}), | x_k^{(\ell)}, Z_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) p(Z_k^{(\ell)} | \childrennode(Z_k^{\ell})) \\
                &= p(X^{(1)}) p(Z^{(L)}) \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}), | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) p(Z_k^{(\ell)} | \childrennode(Z_k^{\ell}))
    \end{align
    }$$
\end{proof}

\newcommand{\taxonomy}{\mathcal{T}}

\begin{proposition}[Expectation-Maximization for Bottom-Up Hidden Tree Markov Model with dirichlet modelization]
    \label{proposition:EM_bottom_up_HTMM_dirichlet}
    Let $\taxonomy$ a global taxonomy.
    Define $(X_i, Z_i)_{1 \leq i \leq n}$ i.i.d latent taxa abundance samples over $\taxonomy$.
    We assume the following framework:
    \begin{itemize}
        \item $Z$ is a bottom-up HTMM with killing state $\killingstate$ so that:
        $$
        \begin{align}
            &p(Z^{(\ell)} = \nu) = \pi_{\nu} \\
            &p(Z_k^{(\ell)} = s | \childrennode(Z_k^{(\ell)}) = \nu) = t_{\nu, s}^{(\ell)}
        \end{align}
        $$
        \item $X$ is a top-down markov tree conditionally to $Z$:
        $$
        \begin{align}
            &p(X^{(1)}) = \delta_{e_1}(X^{(1)}) \\
            &p(X) = p(X^{(1)}) \prod_{\ell = 1}^{L-1} p(X^{(\ell+1)} | X^{(\ell)}) \\
            &p(X^{(\ell+1)} | X^{(\ell)}) = \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)})
        \end{align}
        $$
        \item The conditional distribution of $X$ knowing $Z$ is characterised as follows:
        $$
        p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)}) = \nu) = x_k^{(\ell)} \mathcal{D}(\alpha_{k,\nu}^{(\ell)})
        $$
        $\alpha_{k,\nu}^{(\ell)}$ are the dirichlet parameters of the distribution, which are vectors of dimension $$\#\{j \in \nu | j \neq \killingstate\}$$ as they
        only impact the nodes that are still alive according to $Z$.
    \end{itemize}

    Then we can run an Expectation-Maximization algorithm to compute the optimal parameters of that model onto the dataset, for which the steps are described as follows:

    \medskip

    \textbf{Expectation step}: \\

    For all possible $Z$ over the taxonomy $\taxonomy$, we can explicitly compute $p(Z_i = Z | X_i) = \tau_{iZ}$:

    $$
    \tau_{iZ} = \frac{\delta_{e_1}(X_i^{(1)}) p(Z^{(L)}) \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_{i,k}^{(\ell)}) | x_{i,k}^{(\ell)}, \childrennode(Z_{k}^(\ell))) p(Z_{k}^{(\ell)}|\childrennode(Z_{k}^{(\ell)}))}
                    {\sum_{\tilde{Z} \sim \taxonomy} \delta_{e_1}(X_i^{(1)}) p(\tilde{Z}^{(L)}) \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_{i,k}^{(\ell)}) | x_{i,k}^{(\ell)}, \childrennode(\tilde{Z}_k^(\ell))) p(\tilde{Z}_k^{(\ell)}|\childrennode(\tilde{Z}_k^{(\ell)}))}
    $$

    \textbf{Maximization step}:
    For all possible $\nu$ over the last layer of the taxonomy $\taxonomy^{(L)}$,
    $$
    \pi_{\nu}^* = \frac{1}{n} \sum_{i=1}^n \sum_{Z \sim \taxonomy} \tau_{iZ} \mathds{1}_{Z^{(L)} = \nu}
    $$

    For all given layer $(\ell)$, consider $\nu$ a the possible hidden states of a group of children nodes of the taxonomy $\taxonomy$  at layer $(\ell)$,
    and $s$ a possible state for the parent of the said group of children,
    $$
    t_{\nu, s}^{(\ell)}^* = \frac{1}{\sum_{i=1}^n \sum_{Z \sim \taxonomy} \mathds{1}_{\childrennode(Z_k^{(\ell)}) = \nu} \tau_{iZ}} \sum_{i=1}^n \sum_{Z \sim \taxonomy} \mathds{1}_{Z_k^{(\ell)} = s} \mathds{1}_{\childrennode(Z_k^{(\ell)}) = \nu} \tau_{iZ}
    $$

    For all parent node indexed by $(\ell, k)$, $\nu$ a possible hidden state vector over $\childrennode(Z_k^{(\ell)})$,
    the optimal dirichlet parameter is given by the following fixed point algorithm:
    $$
    \left[\alpha_{k,\nu}^{(\ell)}\right]_j \leftarrow \psi^{-1}\left(\frac{\sum_{i=1}^n \sum_{Z \sim \taxonomy} \tau_{iZ} \mathds{1}_{\childrennode(Z_k^{(\ell)}) = \nu} \log \frac{\childrennode(x_k^{(\ell)})_j}{x_k^{(\ell)}}}
                                                                        {\sum_{i=1}^n \sum_{Z \sim \taxonomy} \tau_{iZ} \mathds{1}_{\childrennode(Z_k^{(\ell)}) = \nu}}
                                                                    + \psi\left(\sum_{j} \left[\alpha_{k,\nu}^{(\ell)} \right]_j\right) \right)
    $$

\end{proposition}

\begin{proof}

    We start by showing the \textbf{Expectation step}:

    \medskip

    Using Bayes formula, we get:
    $$
    \begin{align}
        \tau_{iZ} &= p(Z | X_i) \\
                    &= \frac{p(Z, X_i)}{p(X_i)} \\
                    &= \frac{p(Z, X_i)}{\sum_{\tilde{Z} \sim \taxonomy} p(\tilde{Z}, X_i)}
    \end{align}
    $$

    Using proposition \ref{proposition:likelihood_bottom_up_htmm_taxaabundance_data} to compute the joint likelihood, we get the wanted result.

    \medskip

    We now look into the \textbf{Maximization step}.
    Recall the EM optimization objective:
    \begin{equation}
        \begin{align}
            \theta^* = arg \max_{\theta} Q(\widehat{\theta}, \theta)
        \end{align}
    \end{equation}

    where $Q(\widehat{\theta}, \theta)$ is given by:
    $$
    \begin{align}
        Q(\widehat{\theta}, \theta) &= \mathbb{E}_{p_{\widehat{\theta}}(Z | X)}[\log p_{\theta}(X, Z) | X] \\
                                    &= \sum_{Z \sim \taxonomy} \sum_{i=1}^n \log p_{\theta}(X_i, Z_i = Z) \tau_{iZ} \\
                                    &= \sum_{Z \sim \taxonomy} \sum_{i=1}^n \left( \log \pi_{Z^{(L)}} + \sum_{\ell=1}^{L-1} \sum_{k=1}^{K_{\ell}} \log p_{\alpha_{k, \childrennode(Z_k^{(\ell)})}^{(\ell)}}(\childrennode(x_k^{(\ell)}) | \childrennode(Z_k^{(\ell)}, x_k^{(\ell)})) + \log t^{(\ell)}_{\childrennode(Z_k^{(\ell)}), Z_k^{(\ell)}} \right) \tau_{iZ}
    \end{align}
    $$

    \begin{itemize}
        \item The optimal prior parameters $\pi_{\nu}^*$ are solutions of the following optimization problem:
            $$
            \begin{equation}
                \begin{align}
                    \pi_{\nu}^* = arg &\max_{\pi_{\nu}} \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \log \pi_{Z^{(L)}} \\
                    \textrm{s.t.} \quad & \forall \nu \in \taxonomy^{(L)}, \pi_{\nu} > 0 \\
                                        & \sum_{Z \sim \taxonomy} \pi_{Z^{(L)}} = 1
                \end{align}
            \end{equation}
            $$

            This convex problem is solvable through Lagrange duality, for which the key KKT conditions are the following:
            $$
            \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \mathds{1}_{Z^{(L)} = \nu} \frac{1}{\pi_{\nu}} - \psi + \phi = 0
            $$

            Setting $\phi = 0$, we get:
            $$
            \pi_{\nu} = \frac{1}{\psi} \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \mathds{1}_{Z^{(L)} = \nu}
            $$

            Recall the summation constraint over the $\pi_{\nu}$,
            $$\sum_{Z \sim \taxonomy} \pi_{Z^{(L)}} = 1$$

            Which implies:
            $$
            \begin{align}
                \psi &= \sum_{Z \sim \taxonomy} \sum_{Z' \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \mathds{1}_{Z^{(L)} = \nu} \\
                    &= \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \\
                    &= \sum_{i=1}^n 1 \\
                    &= n
            \end{align}
            $$

            Which gives us the wanted formula.

        \item The optimal transition parameters $t_{\nu,s}^{(\ell)}^*$ are solutions of the following optimization problem:
            $$
            \begin{equation}
                \begin{align}
                    t_{\nu,s}^{(\ell)}^* = arg &\max_{t_{\nu,s}^{(\ell)}} \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \sum_{\ell=1}^{L-1} \sum_{k=1}^{K_{\ell}} \log t^{(\ell)}_{\childrennode(Z_k^{(\ell)}), Z_k^{(\ell)}} \\
                    \textrm{s.t.} \quad &\forall s \in \mathcal{S}(Z_k^{(\ell)}), t_{\nu, s}^{(\ell)} > 0 \\
                                        & \sum_{s \in \mathcal{S}(Z_k^{(\ell)})} t_{\nu,s}^{(\ell)} = 1
                \end{align}
            \end{equation}
            $$

            As previously, this problem can be solved through Lagrange duality. Using a similar reasoning, we get the normalization value for $\psi$ given by:
            $$
            \psi = \sum_{Z \sim \taxonomy} \sum_{i=1}^n \mathds{1}_{\childrennode(Z_k^{(\ell)}) = \nu} \tau_{iZ}
            $$

            Which gives us the objective result.

        \item The optimal abundance parameters $\alpha_{k,\nu}^{(\ell)}^*$, using proposition \ref{proposition:law_of_aX} on the conditional distribution,
            the parameters are solutions of the following optimization problem:
            $$
            \begin{equation}
                \begin{align}
                    \left[\alpha_{k, \nu}^{(\ell)}\right]_j^* = arg &\max_{\left[\alpha_{k,\nu}^{(\ell)}\right]} \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \sum_{\ell=1}^{L-1} \sum_{k=1}^{K_{\ell}} \log \frac{1}{x_k^{(\ell)}} f_{\alpha_{k, \childrennode(Z_k^{(\ell)})}^{(\ell)}}\left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right)\\
                    \textrm{s.t.} \quad &\forall j, \left[\alpha_{k, \nu}^{(\ell)}\right]_j > 0
                \end{align}
            \end{equation}
            $$

            Deriving the objective we get:
            $$
            \begin{align}
                \frac{\partial Q(\widehat{\theta}, \theta)}{\partial \left[\alpha_{k, \nu}^{(\ell)}\right]_j} &= \sum_{Z \sim \taxonomy} \sum_{i=1}^n \tau_{iZ} \mathds{1}_{\childrennode(Z_k^{(\ell)})=\nu} \left(\psi\left(\sum_j\left[\alpha_{k,\nu}^{(\ell)}\right]_j \right) - \psi\left( \left[\alpha_{k,\nu}^{(\ell)}\right]_j\right) + \log \frac{\childrennode(x_k^{(\ell)})_j}{x_k^{(\ell)}}\right)
            \end{align}
            $$

            Looking for $0$ valued gradient and using the fixed point algorithm provided by \cite{dirichlet_digamma_trick}, we obtain the fix point algorithm we want.
    \end{itemize}

\end{proof}

\begin{proposition}[Likelihood under-parameterized Bottom-Up HTMM]
    \label{proposition:likelihood_underparameterized_bottomup_htmm}
    Assume $Z$ is a HTMM over the taxonomy $\taxonomy$, $X$ a taxa-abundance sample over $\taxonomy$.
    Assume that the following holds:
    \begin{itemize}
        \item Let $S \in \mathbb{N}$ the number of possible hidden states, at each layer $(\ell)$ we introduce the parameter $\pi^{(\ell)} \in \mathbb{R}^{S}$, which defines the aggregation parameter $\omega_k^{(\ell)}$:
        $$
        \omega_k^{(\ell)} = \sum_{z \in \childrennode(Z_k^{(\ell)})} \sum_{s=1}^{S} \mathds{1}_{z = e_s} \pi^{(\ell)}_s e_s
        $$
        where $e_s$ denotes the embedding of the state $s$ into a vector representation (one-hot encoding for instance).
        \item $Z$ is bottom-up HTMM with killing state $\killingstate$, for which the transition probabilities are defined through $\omega_k^{(\ell)}$:
        $$
        \begin{align}
            &Z_k^{(L)} \sim Cat(\pi^{(L)}) \\
            &Z_k^{(\ell)} | \childrennode(Z_k^{(\ell)}) \sim Cat(softmax(\omega_k^{(\ell)}))
        \end{align}
        $$
        \item $X$ is a top-down markov tree conditionally to $Z$:
        $$
        \begin{align}
            &p(X^{(1)}) = \delta_{e_1}(X^{(1)}) \\
            &p(X) = p(X^{(1)}) \prod_{\ell = 1}^{L-1} p(X^{(\ell+1)} | X^{(\ell)}) \\
            &p(X^{(\ell+1)} | X^{(\ell)}) = \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)})
        \end{align}
        $$
        \item The conditional distribution of $X$ knowing $Z$ is characterised as follows, for the nodes that have at least $2$ children and that are not at hidden state $\killingstate$:
        $$
        p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(z_k^{(\ell)}) = \nu) = x_k^{(\ell)} \mathcal{D}(\alpha_{k}^{(\ell)} \odot \nu)
        $$

        Otherwise if $x_k^{(\ell)}$ has 1 child, its abundance value is transmitted deterministicly to his child if it is not at state $\killingstate$, otherwise the value is 0.
    \end{itemize}

    Then the likelihood of such model is given by:
    \footnotesize
    $$
    \begin{align}
        p(X, Z) = \delta_{e_1}(X^{(1)}) \prod_{k=1}^{K_L} \pi_{Z_k^{(\ell)}}^{(L)} \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}}
                            &\delta_{0}(\childrennode(x_k^{(\ell)}))^{\mathds{1}_{Z_k^{(\ell)} = \killingstate}} \\
                            &\delta_{x_k^{(\ell)}}(\childrennode(x_k^{(\ell)}))^{\mathds{1}_{\#\childrennode(Z_k^{(\ell)}) = 1} \mathds{1}_{Z_k^{(\ell)} \neq \killingstate}} \\
                            &\left( \frac{1}{x_k^{(\ell)}} f_{\alpha_k^{(\ell)} \odot \childrennode(Z_k^{(\ell)})}\left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right) \right)^{\mathds{1}_{\#\childrennode(Z_k^{(\ell)}) > 1} \mathds{1}_{Z_k^{(\ell)} \neq \killingstate}} softmax(\omega_k^{(\ell)})_{Z_k^{(\ell)}}
    \end{align}

    $$
    \normalsize
\end{proposition}

\begin{proof}

    $$
    \begin{align}
        p(X, Z) &= p(X^{(1)}, Z^{(1)}, \dots, X^{(L)}, Z^{(L)}) \\
                &= \delta_1(X^{(1)}_1) p(Z^{(L)}) \prod_{\ell=1}^{L-1} p(X^{(\ell+1)}, Z^{(\ell)} | X^{(\ell)}, Z^{(\ell+1)}) \\
                &= \delta_1(X^{(1)}_1) \prod_{k=1}^{K_L} \pi_{Z_k^{L}}^{(L)} \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) p(Z_k^{(\ell)} | \childrennode(Z_k^{(\ell)})) \\
        &= \delta_1(X^{(1)}_1) \prod_{k=1}^{K_L} \pi_{Z_k^{L}}^{(L)} \prod_{\ell=1}^{L-1} \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) softmax(w_k^{(\ell)})_{Z_k^{(\ell)}}\\
    \end{align}
    $$

    Notice three possible different scenarios:
    \begin{itemize}
        \item If the node is being killed by the HTMM through $\killingstate$:
            $$p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) = \delta_{0}(\childrennode(x_k^{(\ell)}))$$

        \item If the node has only one child and that it is not killed by the HTMM:
            $$p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(Z_k^{(\ell)})) = \delta_{x_k^{(\ell)}}(\childrennode(x_k^{(\ell)}))$$

        \item Finally, if the children has two children or more and that it is not killed by the HTMM, using the proposition \ref{proposition:law_of_aX}:
            $$p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)} = \frac{1}{x_k^{(\ell)}} f_{\alpha_k^{(\ell)} \odot \childrennode(Z_k^{(\ell)})}\left(\frac{\childrennode(x_k^{(\ell)})}{x_k^{(\ell)}}\right)$$
    \end{itemize}

    Marking these three cases through the corresponding indicator functions we obtain the wanted result.

\end{proof}

\begin{proposition}(Expectation-Maximization for under parameterized Bottom-Up HTMM model)
    \label{proposition:EM_underparameterized_bottomup_HTMM}
    We set ourselves in the framework of \ref{proposition:likelihood_underparameterized_bottomup_htmm}. \\

    Let $(X_i, Z_i)_{1 \leq i \leq n}$ a dataset of respectively taxa-abundance data and hidden functional states defined on the taxonomy $\taxonomy$.
    We denote by $\mathcal{S}(\taxonomy)$ the set of possible HTMM realization over $\taxonomy$. \\

    Assume that we want to find the $\theta^*$ MLE of the under parameterized bottom-up HTMM model over that dataset, then we provide an Expectation-Maximization approach to compute it: \\
    
    \textbf{Expectation-step}: \\

    Denote $\tau_{iZ} = p(Z_i | X_i)$, then this quantity is given by:
    $$
    \tau_{iZ} = \frac{p(Z_i, X_i)}{\sum_{Z \in \mathcal{S}(\taxonomy)}p(Z, X_i)}
    $$

    where $p(Z, X)$ is explicitly provided in proposition \ref{proposition:likelihood_underparameterized_bottomup_htmm}

    \textbf{Maximization-step}: \\

    The optimal prior parameter of the HTMM is given by:
    $$
    \pi_{s}^{(L)}^* = \dots
    $$

    The optimal transition parameters ($1 < \ell < L$) of the HTMM are given by:
    $$
    \pi_{s}^{(\ell)}^* = \dots
    $$

    The optimal abundance parameters are obtained through the following fixed point algorithm:
    $$
    \alpha_k^{(\ell)} \leftarrow \psi^{(-1)}\left( \dots \right)
    $$
    
\end{proposition}

\begin{proof}
\end{proof}