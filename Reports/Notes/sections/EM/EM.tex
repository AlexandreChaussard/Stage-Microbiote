\section{Expectation-Maximization}

\subsection{Overview}

The Expectation-Maximization (EM) algorithm, first introduced in \cite{expectation_maximization_source}, is vast class of latent models.
It is based on the following smart decomposition of the data:
$$
\log p_{\theta}(X) = \underbrace{\mathbb{E}_{p_{\widehat{\theta}}(Z|X)}[\log p_{\theta}(X,Z)|X]}_{Q(\widehat{\theta}, \theta)} - \mathbb{E}_{p_{\widehat{\theta}}(Z|X)}[\log p_{\theta}(Z|X)|X]
$$

The idea behind this decomposition is that $\log p_{\theta}(X)$ is generally not tractable since it's an integral, while the complete likelihood $p_{\theta}(X,Z)$ is generally manageable.
Note that since $\log p_{\theta}(X)$ can not be computed, $\log p_{\theta}(Z|X)$ can't either by extension.
Hence, we introduce $\log p_{\widehat{\theta}}(Z|X)$ where $\widehat{\theta}$ is the maximum likelihood estimator of $\theta$:
$$
\widehat{\theta} = arg\max_{\theta} \log p_{\theta}(X)
$$

The main trick of the EM algorithm relies in the idea that $Q(\widehat{\theta}, \theta)$ is sufficient to compute a maximum likelihood estimator of $\theta$.
Indeed, consider the following algorithm:
\begin{algorithm}[H]
    \caption{Expectation-Maximization}
    \begin{algorithmic}
        \REQUIRE $\widehat{\theta}$
        \STATE Repeat until convergence
            \STATE \quad \textbf{Expectation}: compute $p_{\widehat{\theta}}(Z|X)$ to compute $Q(\widehat{\theta}, .)$
            \STATE \quad \textbf{Maximization}: $\widehat{\theta} = arg\max_{\theta} Q(\widehat{\theta}, \theta)$
    \end{algorithmic}

    \textbf{Output:} $\widehat{\theta}$

    \label{alg:em_algo}
\end{algorithm}

If we denote by $\widehat{\theta}^h$ the iterates of this algorithm, one can show using Jensen's inequality that:
$$
\log p_{\widehat{\theta}^{h+1}}(X) \geq \log p_{\widehat{\theta}^h}(X)
$$

As a result, the EM algorithm maximizes the likelihood, producing an estimator $\widehat{\theta}$ that is an MLE of $\theta$.
Note that we don't have a convergence certainty towards the best maximizer of the likelihood, only to a local maxima.
Hence, the EM algorithm is heavily sensitive to the initialization we pick. \\

All is required now is to choose a latent model so we can perform the EM algorithm, meaning that we have to define the distributions of the followings:
\begin{itemize}
    \item $Z \sim \mathcal{B}(K, \pi)$, conveniently set to a binomial of parameter $\pi$ so that it's discrete and simple to manage.
    \item $X|Z=k \sim p_{\gamma(k)}(X|Z=k)$, which is where we have the most choice to make.
\end{itemize}

In such models, $\theta = (\pi, \gamma(0), ..., \gamma(K))$.
In the next section, we will study a specific fork architecture of the gaussian mixture case.