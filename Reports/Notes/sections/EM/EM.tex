\section{Expectation-Maximization}

\subsection{Overview}

The Expectation-Maximization (EM) algorithm, first introduced in \cite{expectation_maximization_source}, is vast class of latent models.
It is based on the following smart decomposition of the data:
$$
\log p_{\theta}(X) = \underbrace{\mathbb{E}_{p_{\widehat{\theta}}(Z|X)}[\log p_{\theta}(X,Z)|X]}_{Q(\widehat{\theta}, \theta)} - \mathbb{E}_{p_{\widehat{\theta}}(Z|X)}[\log p_{\theta}(Z|X)|X]
$$

The idea behind this decomposition is that $\log p_{\theta}(X)$ is generally not tractable since it's an integral, while the complete likelihood $p_{\theta}(X,Z)$ is generally manageable.
Note that since $\log p_{\theta}(X)$ can not be computed, $\log p_{\theta}(Z|X)$ can't either by extension.
Hence, we introduce $\log p_{\widehat{\theta}}(Z|X)$ where $\widehat{\theta}$ is the maximum likelihood estimator of $\theta$:
$$
\widehat{\theta} = arg\max_{\theta} \log p_{\theta}(X)
$$

The main trick of the EM algorithm relies in the idea that $Q(\widehat{\theta}, \theta)$ is sufficient to compute a maximum likelihood estimator of $\theta$.
Indeed, consider the following algorithm:
\begin{algorithm}[H]
    \caption{Expectation-Maximization}
    \begin{algorithmic}
        \REQUIRE $\widehat{\theta}$
        \STATE Repeat until convergence
            \STATE \quad \textbf{Expectation}: compute $p_{\widehat{\theta}}(Z|X)$ to compute $Q(\widehat{\theta}, .)$
            \STATE \quad \textbf{Maximization}: $\widehat{\theta} = arg\max_{\theta} Q(\widehat{\theta}, \theta)$
    \end{algorithmic}

    \textbf{Output:} $\widehat{\theta}$

    \label{alg:em_algo}
\end{algorithm}

If we denote by $\widehat{\theta}^h$ the iterates of this algorithm, one can show using Jensen's inequality that:
$$
\log p_{\widehat{\theta}^{h+1}}(X) \geq \log p_{\widehat{\theta}^h}(X)
$$

As a result, the EM algorithm maximizes the likelihood, producing an estimator $\widehat{\theta}$ that is an MLE of $\theta$.
Note that we don't have a convergence certainty towards the best maximizer of the likelihood, only to a local maxima.
Hence, the EM algorithm is heavily sensitive to the initialization we pick. \\

All is required now is to choose a latent model so we can perform the EM algorithm, meaning that we have to define the distributions of the followings:
\begin{itemize}
    \item $Z \sim \mathcal{B}(K, \pi)$, conveniently set to a binomial of parameter $\pi$ so that it's discrete and simple to manage.
    \item $X|Z=k \sim p_{\gamma(k)}(X|Z=k)$, which is where we have the most choice to make.
\end{itemize}

In such models, $\theta = (\pi, \gamma(0), ..., \gamma(K))$.
In the next section, we will study a specific fork architecture of the gaussian mixture case.

\subsection{Gaussian Mixture Linear Classifier}

\subsubsection{Framework and computations}
Consider the case for which we observe $(X_i, Y_i)_{1 \leq i \leq n}$ i.i.d samples, where $X_i$ denotes a feature vector and $Y_i$ a label in a classification framework.
We aim at introducing a linear classifier that exploits a latent structure over $(X, Y)$, so that we have the following latent model:
\begin{itemize}
    \item $Z_i \sim \mathcal{B}(K, \pi)$, we note $\pi_k = \mathbb{P}(Z_i = k)$
    \item $X_i|Z_i=k \sim \mathcal{N}(\mu_k, \sigma_k I)$, we denote by $f_k(X_i)$ its density.
    \item $\mathbb{P}(Y_i = 1 | X_i, Z_i=k) = \sigma(W_{e,k}^T e_k + W_{x,k}^T X_i) = p_k(X_i)$, where $e_k$ denotes a vector from the canonical basis of $\mathbb{R}^K$.
\end{itemize}

As we want to perform the EM algorithm find an MLE of
$$
\theta = (\pi, \mu_1, ..., \mu_K, \sigma_1, ..., \sigma_K, W_{e,1}, ..., W_{e,K}, W_{x,1}, ..., W_{x,K})
$$
we start by computing the \textbf{expectation} step by assessing $p_{\widehat{\theta}}(Z_i = k|X_i,Y_i)$, using Bayes rules ($\hat{\pi}, \hat{p}, \hat{f}$ signify that we evaluate these quantities using the current estimate $\widehat{\theta}$):
$$
\begin{align}
    p_{\widehat{\theta}}(Z_i = k|X_i, Y_i) &= \frac{\hat{\pi}_k \hat{f}_k(X_i) \left(Y_i \hat{p}_k(X_i) + (1 - Y_i) (1 - \hat{p}_k(X_i)) \right)}{\sum_{j=1}^K \hat{\pi}_j \hat{f}_j(X_i) \left(Y_i \hat{p}_j(X_i) + (1 - Y_i) (1 - \hat{p}_j(X_i)) \right)}
    \\&= \tau_{ik}
\end{align}
$$

Now, we can safely evaluate $Q(\widehat{\theta}, \theta)$ for any $\theta$:
$$
\begin{align}
    Q(\widehat{\theta}, \theta) &= \mathbb{E}_{p_{\widehat{\theta}}(Z|X)}[\log p_{\theta}(X, Y, Z) | X] \\
                                &= \sum_{i=1}^n \sum_{k=0}^K \log p_{\theta}(X_i, Y_i, Z_i = k) \tau_{ik} \\
                                &= \sum_{i=1}^n \sum_{k=0}^K (\log \pi_k + \log f_k(X_i) + Y_i \log p_k(X_i) + (1 - Y_i) \log (1-p_k(X_i))) \tau_{ik} \\
\end{align}
$$

The \textbf{maximization} step now consists in deriving $Q(\widehat{\theta}, \theta)$ regarding each parameters in $\theta$
so that we obtain an either explicit value or iterative procedure to compute the next iterate of $\widehat{\theta}$.
\begin{itemize}
    \item Maximization regarding $\pi_k$ under constraint that $\sum_{i=1}^n \pi_k = 1$ can be solved explicitly using Lagrange duality:
          $$
          \pi_k^* = \frac{1}{n} \sum \tau_{ik}
          $$
    \item Maximization regarding $(\mu_k, sigma_k)$ is given by the maximum of likelihood estimator on $\sum_{i=1}^n \tau_{ik} \log f_{k}(X_i)$:
          $$
          \begin{align}
              \mu_k^* &= \frac{1}{\sum_{i=1}^n \tau_{ik}} \sum_{i=1}^n \tau_{ik} X_i \\
              \sigma_k^* &= \frac{1}{\sum_{i=1}^n \tau_{ik}} \sum_{i=1}^n \tau_{ik} (X_i - \mu_k^*)^2
          \end{align}
          $$
    \item Maximization regarding $(W_{e,k}, W_{x,k})$ is not explicit, and requires a fixed point algorithm like gradient descent to determine an estimate of the optimal parameters.
          The iterations using full batch gradient descent are given below, with learning rate $\alpha$:
            $$
            \begin{align}
                W_{e,k}^{l+1} &\leftarrow W_{e,k}^{l} - \alpha \sum_{i=1}^n (p_k(X_i) - Y_i) \tau_{ik} e_k \\
                W_{x,k}^{l+1} &\leftarrow  W_{x,k}^{l} - \alpha \sum_{i=1}^n (p_k(X_i) - Y_i) \tau_{ik} X_i
            \end{align}
            $$
          Each iteration should be confronted to the maximization criterion, so that each iterate improves $Q(\widehat{\theta}, \theta)$:
          $$
          Q(\widehat{\theta}^{l+1}, \theta) \geq Q(\widehat{\theta}^{l}, \theta)
          $$
          In the end, only the best improvement iterate is kept for $W_{e,k}^*$ and $W_{x,k}^*$. Note that other methods could be used like SGD or CMAES as implemented during the internship.
\end{itemize}

After the maximization, we can update $\widehat{\theta}$ with the previously computed parameters, and redo the (E) and (M) steps up to convergence.
The convergence can be measured relatively to $ Q(\widehat{\theta}, \theta)$, so that for a threshold $\epsilon$, we can use the following stopping criterion:
$$
\frac{|Q(\widehat{\theta}^{h+1}, \theta) - Q(\widehat{\theta}^{h}, \theta)|}{|Q(\widehat{\theta}^{h}, \theta)|} \leq \epsilon
$$

We now have a ready-to-go Gaussian Mixture Linear Classifier that we can benchmark on an suited dataset against other common methods.

\subsubsection{Dataset generation}