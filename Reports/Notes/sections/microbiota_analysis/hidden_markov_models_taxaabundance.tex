\subsection{Hidden Markov Models: functional modelization}

\subsubsection{Context of Hidden Markov Models and Hidden Tree Markov Models}

One key goal of our analysis of the microbiota is to highlight functional groups of
bacteria in a studied context (reaction to treatment, overall view, disease tracking, post-surgery reaction, ...).
Unveiling such groups can be done through various processes, namely by using latent variables.

\medskip

Previously, we have worked with i.i.d latent variables, meaning that the hidden representation does not
implement any typical graph structure that would model latent dependencies between the bacteria (i.e., functional areas).
A typical latent model that would admit the simplest graph structure of a chain is given by the Hidden Markov Models.

\medskip

In a nutshell, Hidden Markov Models (HMM) are a class of generative models such that the latent variable follows a Markov Chain.
Formally, for an observed variable $Y = (Y_1, \dots, Y_n)$, $Z = (Z_1, \dots, Z_n)$ is a discrete HMM relatively to $Y$ if we have the following:
\begin{itemize}
    \item $Z$ is a Markov Chain (MC) with discrete states:
         $$
         \mathbb{P}(Z_{i+1} | Z_i, Z_{i-1}, \dots, Z_1) = \mathbb{P}(Z_{i+1} | Z_i)
         $$
    \item We define a prior on the seed of the MC:
         $$\forall c \in \{1, \dots, C\}, \mathbb{P}(Z_1 = c) = \pi_c$$

    \item We define a transition probability on the states of the MC:
        $$\forall c,k \in \{1, \dots, C\}^2, \mathbb{P}(Z_{i+1} = c | Z_i = k) = t_{k,c}^{(i+1)}$$

    \item We define an emission probability for $Y$ to be in a given state when observing $Z$:
        $$
        P(Y_i \in A | Z_i = c) = a_c(A)
        $$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[scale=.25]{images/graph_dependency_HMM}
    \caption{Dependency graph of a HMM ($Z$ is the latent MC, $Y$ the observation)}
    \label{fig:graph_dependency_hmm}
\end{figure}

As a result, we can use HMM to model hidden functional groups of bacteria from one layer to the other.

This idea can even be extended to the specific tree-like structure of taxa-abundance data, introducing
the wider class of models given by the Hidden Tree Markov Model (HTMM) \cite{hidden_tree_markov_models}.
The HTMM provides a dependency graph that accounts for the tree structure, so that instead of a chain, we get
a Markov tree.
The following figure illustrates a given HTMM, precisely a top-down architecture:

\begin{figure}[H]
    \centering
    \includegraphics[scale=.5]{images/top_down_htmm_example}
    \caption{Top-down HTMM example: $Z$ represents the latent Markov tree,
        $X$ the observed abundance of a given bacteria in the taxonomic tree}
    \label{fig:top_down_htmm_example}
\end{figure}

The mathematical framework is then slightly different for top-down HTMM:
\begin{itemize}
    \item $Z$ is a Markov Tree (MT):
        $$\mathbb{P}\left(Z_{k}^{(l)} = c | Z_{0}^{(\ell)}, \dots, Z_{K_{\ell}}^{(\ell)}, \nodeparent(Z_{0}^{(\ell)}), \dots, \nodeparent(Z_{K_{\ell}}^{(\ell)}), \dots, Z_1^{(1)} \right) = \mathbb{P}\left(Z_{k}^{(\ell)} | \nodeparent(Z_{k}^{(\ell)})\right)$$
    \item We define a prior on the first layer of the MT, relatively to a given tree $T$:
        $$
        \mathbb{P}\left(Z_1^{(1)} = j | T^{(1)}\right) = a_j
        $$
    \item We define the transition probabilities of the MT, relatively to a given tree $T$:
        $$\mathbb{P}\left(\childrennode(Z_k^{(\ell)}) = \nu | Z_k^{(\ell)} = j, T^{(\ell+1)}\right) = t_{\nu,j}^{(\ell)}$$
\end{itemize}

Notice that this model would be tree dependent, as we want to attach existing nodes to a given hidden state, to generate
the abundance accordingly.
One could think of a tree generator model by applying the HTMM onto the global taxonomy directly, and defining an "empty"
state that would turn off the specific nodes in the final representation.

\medskip

Some recent work on HTMM have been showing that top-down modelling captures
less discriminent detail and conditional dependencies between the nodes than the bottom-up architecture \cite{bottom_up_superiority_hidden_tree_markov_models}.
In this first study though, we will focus on top-down models as a first try, as it makes the generation more natural (since we can not generate from bottom to up for the abundance).
Note that bottom-up modeling is still possible, but we can not generate sequentially the abundance with $Z$.
We would need to generate $Z$ entirely, and then generate $X$ in a top-down fashion.

\subsubsection{Tree generation: layer-wise Hidden Markov Model}

In this section, we aim at modeling the distribution of the trees $(T_i)_{1 \leq i \leq n}$ through a layer-wise Hidden Markov Model.
The following figure illustrates the dependency between the latent markov chain and the tree:
\begin{figure}[H]
    \centering
    \includegraphics[scale=.4]{images/layer_wise_hidden_markov_model_tree}
    \caption{Layer-wise hidden markov model dependency graph: $Z$ is the HMM, $T^{(\ell)}$ is the layer $(\ell)$ of the tree}
    \label{fig:layer_wise_hmm_tree}
\end{figure}

As illustrated on figure \ref{fig:layer_wise_hmm_tree} and by nature of our taxonomic data,
the layer of the tree follow at least a Markov process (it could be deeper than markovian).
Conversely to the previous situations, the latent variable now follows a markov chain as well, which enables to
group the layers of the tree with a dependence on the parent's layer group.

\medskip

Furthermore, in that approach, we decide to model the layers rather than the nodes to introduce dependency between
the nodes of a given layer, so we do not look into the $u_k^{(\ell)}$ anymore but directly into the $T^{(\ell)}$.

\medskip

Concisely, the mathematical framework can be written as follow:
\begin{itemize}
    \item $Z$ follows a discrete MC:
        $$
        \begin{align}
            &\mathbb{P}\left(Z^{(1)} = c\right) = a_c \\
            &\mathbb{P}\left(Z^{(\ell+1)} = k | Z^{(\ell)} = c\right) = t_{c,k}^{(\ell+1)}
        \end{align}
        $$
    \item $T$ is markovian and parameterized conditionally to $Z$.
          We denote by $\childrennode(T^{(\ell)})$ the set of plausible activation vectors at layer $(\ell+1)$ in the taxonomy, relatively to
          the parents activation given by $T^{(\ell)}$. We then model the emission probability by:
        $$\mathbb{P}(T^{(\ell+1)} = \omega | Z^{(\ell+1)} = c, T^{(\ell)}) = \frac{\pi_c(\omega) \mathds{1}_{\omega \in \childrennode(T^{(\ell)})}}{\sum_{\nu \in \childrennode(T^{\ell})} \pi_c(\nu)}$$

\end{itemize}

One may notice that such modelisation on $T$ will require a large number of parameters.
Indeed, the set of plausible activation vectors at layer $(\ell+1)$ relatively to $T^{(\ell)}$ can be quite big, as for each activate node $u_k^{(\ell)}$ in $T^{(\ell)}$
we get $2^{|\childrennode(u_k^{(\ell)})|} - 1$ plausible activated vectors of its children ($-1$ comes from the fact that we would like to avoid missing values).
We denote by $\mathcal{S}(Z)$ the discrete state space of the hidden markov chain.
Hence, for a given tree $T$, we obtain a total number of parameters of:
$$
n_{\text{params}} = \sum_{\ell=1}^{L}\sum_{k=1}^{K_{\ell}} |\mathcal{S}(Z)| \times \left(2^{|\childrennode(u_k^{(\ell)})|} - 1\right)
$$

As usual for latent data models, variational methods come in handy for learning the model.
Hence, we provide an Expectation-Maximization algorithm to compute the optimal parameters of the prior over the tree
in proposition \ref{proposition:em_layer_wise_hmm_tree}.

\subsubsection{Abundance generation: Hidden Tree Markov Model}

In this section, we aim at modeling the distribution of the abundance data $(X_i)_{1 \leq i \leq n}$ conditionally
to its known corresponding trees $(T_i)_{1 \leq i \leq n}$, using a Hidden Tree Markov Model (HTMM).
The dependency graph is illustrated by the figure \ref{fig:top_down_htmm_example}, giving us the follow mathematical framework:

\begin{itemize}
    \item $Z$ is a discrete Markov Tree (MT):
        \begin{itemize}
            \item We define a prior over the root:
                $$\mathbb{P}(Z_1^{(1)} = c | T^{(1)}) = a_j$$
            \item We define the transition probabilities:
                $$\mathbb{P}(\childrennode(Z_k^{(\ell)}) = \nu | Z_k^{(\ell)} = j, T^{(\ell+1)}) = t_{j,\nu}^{(\ell)}$$
        \end{itemize}
    \item $X$ follows a Markov Tree structure.
        We define the emission probability of the corresponding abundance relatively to the hidden states and tree:
        $$p(\childrennode(x_k^{(\ell)}) | \childrennode(Z_k^{(\ell)}) = \nu, x_k^{(\ell)}, T^{(\ell+1)}) \sim x_k^{(\ell)} \mathcal{D}(\alpha_{\nu,k}^{(\ell)} \odot T^{(\ell+1)}_k)$$
\end{itemize}

The number of parameters of that model is quite large, as it depends on the size of the latent space state and the number of child node
of each parent node in the tree structure.
Indeed, for each node, the corresponding dirichlet parameter is of the size of the number of children (if it has more than 1 child).
Then, we have as many dirichlet parameters as the possible amount of vector $\nu$, for which each coordinate has $|\mathcal{S}(Z)|$ plausible value.
That leads us to the following number of parameters:
$$
n_{\text{params}} = \sum_{\ell=1}^L \sum_{k=1}^{K_{\ell}} |\mathcal{S}(Z)|^{|\childrennode(u_k^{(\ell)})|} \times |\childrennode(u_k^{(\ell)})| \times \mathds{1}_{|\childrennode(u_k^{(\ell)})| > 1}
$$

As previously for the tree generation, we propose an Expectation-Maximization algorithm to compute the optimal parameters of
that HTMM, detailed in proposition \textbf{[TODO]}.

\subsubsection{Bottom-Up Hidden Tree Markov Model: general model}

The previous approach used a Top-Down HTMM (see figure \ref{fig:top_down_htmm_example}), which is a valid approach to model a parent to child dependency only.
Medically speaking, the top-down HTMM can be seen as parent dependent functions: a parent's function would determine the function of its child, which would be independently sorted conditionally to the parent.
In addition to that questionable modelisation, related work from \cite{bottom_up_superiority_hidden_tree_markov_models} as shown that top-down HTMM are capturing
less conditional depencencies between nodes, modeling less distributions than a bottom-up approach. \\

Hence, we are now interested in a bottom-up HTMM, which is illustrated in figure \ref{fig:bottom_up_htmm_example}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=.4]{images/bottom_up_hidden_markov_tree_example}
    \caption{Bottom-down Hidden Tree Markov Model architecture: $Z$ represents the latent space, $X$ a taxa-abundance sample.
    The pointed out sections and arrows represent the dependencies.
    Each node is marked by its layer $(\ell)$ and position within that layer $k$.}
    \label{fig:bottom_up_htmm_example}
\end{figure}

\newcommand{\killingstate}{s_{\emptyset}}

The hidden states $Z$ are taking the graph dependency form of a tree that covers the entire taxonomy, and take value in a discrete set $\mathcal{S(Z)}$.
To take in consideration the sparsity of the taxa-abundance data, we introduce a killing state that we denote as $\killingstate$, which filter out a node and its children when appearing. \\

The general framework is then the following:
\begin{itemize}
    \item $Z$ is bottom-up HTMM with killing state $\killingstate$ so that:
            $$
            \begin{align}
                &p(Z^{(L)} = \nu) = \pi_{\nu} \\
                &p(Z_k^{(\ell)} = s | \childrennode(Z_k^{(\ell)}) = \nu) = t_{\nu, s}^{(\ell)}
            \end{align}
            $$
    \item $X$ is a top-down markov tree conditionally to $Z$:
            $$
            \begin{align}
                &p(X^{(1)}) = \delta_{e_1}(X^{(1)}) \\
                &p(X) = p(X^{(1)}) \prod_{\ell = 1}^{L-1} p(X^{(\ell+1)} | X^{(\ell)}) \\
                &p(X^{(\ell+1)} | X^{(\ell)}) = \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)})
            \end{align}
            $$
    \item The conditional distribution of $X$ knowing $Z$ is characterised as follows:
            $$
            p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(z_k^{(\ell)}) = \nu) = x_k^{(\ell)} \mathcal{D}(\alpha_{k,\nu}^{(\ell)})
            $$
          $\alpha_{k,\nu}^{(\ell)}$ are the dirichlet parameters of the distribution, which are vectors of dimension $$\#\{j \in \nu | j \neq \killingstate\}$$ as they
          only impact the nodes that are still alive according to $Z$.
\end{itemize}

To compute the optimal parameters of that model given a dataset of taxa-abundance data $(X_i)_{1 \leq i \leq n}$, we propose an Expectation-Maximization algorithm
in proposition \ref{proposition:EM_bottom_up_HTMM_dirichlet}.
However, even if this model is covers all types of problems with its general form, it may not be suited to our special use case with very few samples.
Hence, the next section provides an under-parameterized alternative model to fit the framework of low samples datasets.

\subsubsection{Bottom-Up Hidden Tree Markov Model: low sample regime}

The previous bottom-up HTMM model is extremely general, as it provides a parameter for each transition configuration through $t^{(\ell)}_{\childrennode(Z_k^{(\ell)}), Z_k^{(\ell)}}$.
However, training such model onto a small dataset may be difficult and could result in overfitting, as we may highly likely not explore all possible configurations for a node and its children, and that all the configuration do not
impact each other's transition probabilities in the HTMM. \\

Hence, building on the previous model, we introduce a low parameterized model, which may not capture all the expressiveness of the general HTMM, but that could be trainable on a low sample regime.
\begin{itemize}
    \item Let $S \in \mathbb{N}$ the number of possible hidden states, at each layer $(\ell)$ we introduce the parameter $\pi^{(\ell)} \in \mathbb{R}^{S}$, which defines the aggregation parameter $\omega_k^{(\ell)}$:
            $$
            \omega_k^{(\ell)} = \sum_{z \in \childrennode(Z_k^{(\ell)})} \sum_{s=1}^{S} \mathds{1}_{z = e_s} \pi^{(\ell)}_s e_s
            $$
         where $e_s$ denotes the embedding of the state $s$ into a vector representation (one-hot encoding for instance).
    \item $Z$ is bottom-up HTMM with killing state $\killingstate$, for which the transition probabilities are defined through $\omega_k^{(\ell)}$:
    $$
    \begin{align}
        &Z_k^{(L)} \sim Cat(\pi^{(L)}) \\
        &Z_k^{(\ell)} | \childrennode(Z_k^{(\ell)}) \sim Cat(softmax(\omega_k^{(\ell)}))
    \end{align}
    $$
    \item $X$ is a top-down markov tree conditionally to $Z$:
    $$
    \begin{align}
        &p(X^{(1)}) = \delta_{e_1}(X^{(1)}) \\
        &p(X) = p(X^{(1)}) \prod_{\ell = 1}^{L-1} p(X^{(\ell+1)} | X^{(\ell)}) \\
        &p(X^{(\ell+1)} | X^{(\ell)}) = \prod_{k=1}^{K_{\ell}} p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)})
    \end{align}
    $$
    \item The conditional distribution of $X$ knowing $Z$ is characterised as follows, for the nodes that have at least $2$ children:
    $$
    p(\childrennode(x_k^{(\ell)}) | x_k^{(\ell)}, \childrennode(z_k^{(\ell)}) = \nu) = x_k^{(\ell)} \mathcal{D}(\alpha_{k}^{(\ell)} \odot \nu)
    $$
    $\alpha_{k}^{(\ell)}$ are the dirichlet parameters of the distribution, which are vectors of dimension $#\childrennode(Z_k^{(\ell)})$.
    The hadamard product with $\nu$ denotes that we mask the coordinates of $\alpha_k^{(\ell)}$ according to the states that are still alive i.e. different from $e_{\killingstate}$.
    If the node has $1$ child or none, the abundance value has to be the same as the parent.
\end{itemize}

Note that the hidden states are embedding vectors $e_s$ with $s \in \{1, \dots, S\}$.
So, for notation simplicity, we will introduce the following subscript notation.
Given $\theta$ a vector of dimension $S$, $Z$ a random variable taking value in the discrete state set $\mathcal{S}(Z)$ of dimension $S$, the embedding vectors of $\mathcal{S}(Z)$ are denoted by $e_s$:
$$
\theta_{Z} = \sum_{s=1}^{S} \mathds{1}_{Z=e_s} \theta_{s}
$$

Finding the optimal parameters of such model fitting a dataset can be done in the sense of the maximum likelihood, using the proposed Expectation-Maximization algorithm
of proposition ???.