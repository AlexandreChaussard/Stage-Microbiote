\subsection{Variational Tree Walk}

\subsubsection{EM approach}

In first approximation, we would like to use the EM algorithm to try to compute the following latent model:
\begin{itemize}
    \item We assume that we have $n$ individuals observed through $(X_i, T_i)_{1 \leq i \leq n}$.
    \item The maximum depth of a phylogenetic tree is $D$, the maximum number of nodes $N_T$, and the maximum amount of unique species at any level is $U$.
    \item $X_i$ is the abundance matrix of the individual $i$ (image of size $D \times U$) , which is observed.
    \item $T_i$ is the adjacent matrix of the phylogenetic tree of individual $i$, of size $N_T \times N_T$, which is observed.
          We could use any other encoding of a tree (contour function, depth function, ...).
    \item $Z_i$ is a hidden random walk over the tree $T_i$ (discrete vector, permutation of $(1, \dots, N_T)$).
          For a given random walk process over the tree, we denote by $N_{Z}$ the amount of possible generations ($N_Z \leq N_T!$).
          Hence, $p_{\gamma}(Z_i = z | T_i)$ is a known distribution defined by the random walk process.
    \item Taking inspiration from the PixelCNN, our model should learn the distribution $p_{\omega}(X_i | Z_i, T_i)$ as a CNN/RNN that is masked following $Z_i$.
          We also assume that conditionally to $Z_i$, this distribution is independent from the adjacent matrix: $p_{\omega}(X_i | Z_i, T_i) = p_{\omega}(X_i | Z_i)$.
\end{itemize}

Recall from the EM context that we aim at maximizing
$$
Q(\widehat{\theta}, \theta) = \mathbb{E}_{p_{\widehat{\theta}(Z|X,T)}}[\log p_{\theta}(X, Z, T) | X,T]
$$
Hence, we start with the \textbf{expectation} step and try to evaluate $p_{\widehat{\theta}}(Z|X,T)$:
$$
\begin{align}
    \log p_{\widehat{\theta}}(Z_i|X_i,T_i) = \tau_{ik} &= \log \frac{p(T_i) p_{\widehat{\theta}}(Z_i | T_i) p_{\widehat{\theta}}(X_i | Z_i, T_i)}{\sum_{j=1}^{N_Z} p(T_i) p_{\widehat{\theta}}(Z_i=z_j | T_i) p_{\widehat{\theta}}(X_i | Z_i=z_j, T_i)} \\
                                &= \log \frac{p_{\widehat{\theta}}(Z_i | T_i) p_{\widehat{\theta}}(X_i | Z_i)}{\sum_{j=1}^{N_Z} p_{\widehat{\theta}}(Z_i=z_j | T_i) p_{\widehat{\theta}}(X_i | Z_i=z_j)}
\end{align}
$$
Even though all terms seems computable and explicit here, the summation over $N_Z$ can be very large depending on the complexity of the random walk process we pick.
As a result, we will propose a variational approach to overcome this issue in the next section. \\

As it is right now, one could easily compute the \textbf{maximization} step for $Q(\widehat{\theta}, \theta)$ over $\theta = (\gamma, \omega)$, as it is now explicit:
$$
Q(\widehat{\theta}, \theta) = \sum_{i=1}^n (\log p(T_i) + \log p_{\gamma}(Z_i | T_i) + \log p_{\omega}(X_i | Z_i)) \tau_{ik}
$$

\subsubsection{Variational approach}

In this second approach, we aim at giving a variational algorithm to compute a lower bound to the true objective.
To that end, since it can not be computed directly, we propose an estimate of $p_{\theta}(Z|X,T) = q_{\Phi}(Z|X,T)$.
Recall then the ELBO explicited in the VAE general framework:
$$
\log p_{\theta}(X) = \underbrace{\mathbb{E}_{q_{\Phi}(Z|X,T)}\left[ \log \frac{p_{\theta}(X,Z,T)}{q_{\Phi}(Z|X,T)} \right]}_{ELBO(\Phi, \theta)} + D_{KL}[q_{\Phi}(Z|X,T) \Vert p_{\theta}(Z|X,T)]
$$
Such ELBO can be rewritten as:
$$
\begin{align}
    ELBO(\Phi, \theta) &= \mathbb{E}_{q_{\Phi}(Z|X,T)}[\log p_{\theta}(X,Z,T)] - \mathbb{E}_{q_{\Phi}(Z|X,T)}[\log q_{\Phi}(Z|X,T)] \\
                        &= \mathbb{E}_{q_{\Phi}(Z|X,T)}[\log p_{\theta}(X|Z,T)] - \mathbb{E}_{q_{\Phi}(Z|X,T)}\left[ \log \frac{q_{\Phi}(Z|X,T)}{p_{\gamma}(Z,T)} \right] \\
                        &= \mathbb{E}_{q_{\Phi}(Z|X,T)}[\log p_{\theta}(X|Z)] - D_{KL}[q_{\Phi}(Z|X,T) \Vert p_{\gamma}(Z|T)] + \log p(T)
\end{align}
$$

Designing the prior $p_{\gamma}(Z_i | T_i)$ is no easy task though, as it is completely free-form.
Indeed, using a too free-form prior will end up in not using the tree structure at all by creating a completely unrelated chain
(if any leap can be made in the tree, then the obtained random walk will not necessarily exploit the tree structure).
If the the prior is too restrictive, we might be over-exploiting the phylogenetic structure and obtain an average walk over the tree that is not relevant to each unique case.
Hence, we try to design a prior that takes into account the tree structure while still having some entropy so any random walk is still possible (with low probability).\\

For notation purposes, we introduce the random variable $z_k$ that represents the chosen node index at step $k$ of the random walk.
Whatever prior we decide to pick, it feels as most relevant to pick the seed of the chain using a learnable distribution,
so that we have $z_{1} \sim p(\pi_1)$ where $\pi_1 = (\pi_1^{(1)}, \dots, \pi_1^{(N_T)})$ .
We also introduce a norm over the tree $\Vert . \Vert_T$ that will enable us to measure the distance between nodes:
\begin{itemize}
    \item Norm of a given node is given by:
        $$
        \Vert z_k \Vert_T = \text{distance to root node of the tree}
        $$
    \item Distance computation is given by:
          $$
            \Vert z_k - z_j \Vert_T = \left\{
            \begin{array}{ll}
                0 & \mbox{if } i = j \\
                1 & \mbox{if } i \neq j \mbox{ and nodes have the same parent} \\
                \text{number of edges between node $k$ and $j$} & \mbox{otherwise}
            \end{array}
            \right.
          $$
\end{itemize}

The first prior that we propose is a variation of the dirichlet prior, parameterized by $\lambda = (\lambda_1, \dots, \lambda_{N_T})$.
If we denote by $u_k$ a given node indexed by $k$, let $\alpha_j = (\Vert u_k - z_j \Vert_T)_{1 \leq k \leq N_T}$:
$$
\begin{align}
    \pi_{k+1} | z_{k} &\sim \mathcal{D}(\lambda \odot \alpha_k) \\
    z_{k+1} | \pi_{k+1} &\sim p(\pi_{k+1})
\end{align}
$$
Hence, computing $Z_i | T_i$ is given by $Z_i = (z_k)_{k \in \{1, \dots, N_T\}}$ following the previously introduced process:
\begin{algorithm}[H]
    \caption{Tree random walk without replacement}
    \begin{algorithmic}[1]
        \REQUIRE $T_i$ a given tree, $(\pi_1, \lambda)$ distribution parameters
        \STATE Draw $z_{1} \sim p(\pi_1)$
        \STATE Define $Z_i = (z_{1})$
        \For{ $k \in \{2, \dots, N_T\}$ }
            \STATE \quad Draw a distribution $\pi_{k} | z_{k-1} \sim \mathcal{D}(\lambda \odot \alpha_{k-1})$
            \STATE \quad Mask in $\pi_{k}$ the already seen nodes in $Z_i$ to obtain $\tilde{\pi}_{k}$
            \STATE \quad Draw $z_{k} \sim p(\tilde{\pi}_{k})$
            \STATE \quad Update $Z_i = (Z_i, z_k)$
        \EndFor:

        \RETURN{} $Z_i = (z_1, \dots, z_{N_T})$, $\pi = (\pi_1, \dots, \pi_k)$
    \end{algorithmic}


    \label{alg:tree_walk_no_replacement}
\end{algorithm}

Computing $q_{\Phi}(Z_i | X_i, T_i)$ is not so simple at this stage, and it does not have an explicit formula:
$$
\begin{align}
    q_{\Phi}(Z_i = (z_1, \dots, z_{N_T}), \Pi | X_i, T_i) &= p_{\Phi}(z_1, \dots, z_{N_T}, \pi_1, \dots, \pi_{N_T}) \\
                                                                &= p_{\Phi}(z_1, \pi_1) p_{\Phi}(z_2, \pi_2 | z_1, \pi_1) \dots p_{\Phi}(z_{N_T}, \pi_{N_T} | z_{1:N_T-1}, \pi_{1:N_T-1}) \\
                                                                &= p(\pi_1) p(z_1 | \pi_1) \prod_{k=2}^{N_T} p(\pi_{k} | z_{1:k-1}, \pi_{1:k-1}) p(z_{k} | \pi_{1:k}, z_{1:k-1}) \\
                                                                &= p(z_1 | \pi_1) \prod_{k=2}^{N_T} p(\pi_k | z_{k - 1}) p(z_{k} | \pi_{k}, z_{1:k}) \\
                                                                &= p(z_1 | \pi_1) \prod_{k=2}^{N_T} p_{\lambda}(\pi_k ; \alpha_{k - 1}) p(z_{k} | \tilde{\pi}_{k}) \\
\end{align}
$$
Leading to:
$$
\begin{align}
    q_{\Phi}( z | X_i, T_i) &= \int_{\mathbold{\Pi}}  q_{\Phi}((z_1, \dots, z_{N_T}), \pi | X_i, T_i) d\pi \\
                            &= \int_{\mathbold{\Pi}}  p(z_1 | \pi_1) \prod_{k=2}^{N_T} p_{\lambda}(\pi_k ; \alpha_{k - 1}) p(z_{k} | \tilde{\pi}_{k}) d\pi_1 \dots d\pi_{N_T} \\
                            &= \int_{\mathbold{\Pi}_1} \pi_1^{(z_1)} \int_{\mathbold{\Pi}_2} p_{\lambda}(\pi_2;\alpha_1) \mathbbm{1}_{z_2 \notin \{z_1\}} \pi_{2}^{(z_2)} \dots \int_{\mathbold{\Pi}_{N_T}} p_{\lambda}(\pi_{N_T};\alpha_{N_T - 1}) \mathbbm{1}_{z_{N_T} \notin \{z_1, \dots, z_{N_T-1}\}} \pi_{N_T}^{(z_{N_T})} d\pi_{1:N_T} \\
                            &= \prod_{k=2}^{N_T} \mathbbm{1}_{z_{k} \notin \{z_{1}, \dots, z_{k-1}\}} \int_{\mathbold{\Pi}_2} p_{\lambda}(\pi_2;\alpha_1) \pi_{2}^{(z_2)} \dots \int_{\mathbold{\Pi}_{N_T}} p_{\lambda}(\pi_{N_T};\alpha_{N_T - 1}) \pi_{N_T}^{(z_{N_T})} d\pi_{1:N_T} \\
\end{align}
$$
Since we are using the dirichlet distribution, its components are independent so that $p_{\lambda}(\pi_k;\alpha_{k-1}) = \frac{1}{B(\alpha_{k-1})} \prod_{i=1}^{N_T} \left(\pi_k^{(i)}\right)^{\alpha_{k-1}^{(i)} - 1}$.
Hence, using Fubini-Tonelli theorem and the first order moment of the dirichlet distribution, we get for all $k \geq 2$:
$$
\begin{align}
    \int_{\mathbold{\Pi}_k} p_{\lambda}(\pi_k;\alpha_{k-1}) \pi_{k}^{(z_k)} d\pi_{k} &= \int_{\mathbold{\Pi}_k} \frac{1}{B(\alpha_{k-1})} \prod_{i=1}^{N_T} \left(\pi_k^{(i)}\right)^{\alpha_{k-1}^{(i)} - 1} \pi_{k}^{(z_k)} d\pi_{k}^{(1:N_T)} \\
                                                                                     &= \frac{1}{B(\alpha_{k-1})} \int_{\mathbold{\Pi}_k^{(z_k)}} \left(\pi_k^{(z_k)}\right)^{\alpha_{k-1}^{(z_k)}-1} \pi_k^{(z_k)} d\pi_k^{(z_k)} \prod_{i=0, i \neq z_k}^{N_T} \int_{\mathbold{\Pi}_k^{(i)}} \left(\pi_k^{(i)}\right)^{\alpha_{k-1}^{(i)}-1} d\pi_{k}^{(i)} \\
                                                                                     &= \frac{1}{B(\alpha_{k-1})} \int_{\mathbold{\Pi}_k^{(z_k)}} \left(\pi_k^{(z_k)}\right)^{\alpha_{k-1}^{(z_k)}} d\pi_k^{(z_k)} \prod_{i=0, i \neq z_k}^{N_T} \frac{1}{\alpha_{k-1}^{(i)}} \\
                                                                                     &= \frac{1}{B(\alpha_{k-1})} \frac{1}{\alpha_{k-1}^{(z_k)} + 1} \prod_{i=0, i \neq z_k}^{N_T} \frac{1}{\alpha_{k-1}^{(i)}}
\end{align}
$$
Consequently, $q_{\Phi}( z | X_i, T_i)$ can be written as:
$$
q_{\Phi}(Z_i = (z_1, \dots, z_{N_T}) | X_i, T_i) = \prod_{k=2}^{N_T} \mathbbm{1}_{z_{k} \notin \{z_{1}, \dots, z_{k-1}\}} \prod_{k=2}^{N_T} \left[  \frac{1}{B(\alpha_{k-1})} \frac{1}{\alpha_{k-1}^{(z_k)} + 1} \prod_{i=0, i \neq z_k}^{N_T} \frac{1}{\alpha_{k-1}^{(i)}} \right]
$$

Taking inspiration from the VQ-VAE architecture, one might modify the previous algorithm as simple prior sampling denoted by $Z_i^e(T_i)$,
and then a quantization step by taking the $arg\max$ over the output probabilities sampled:
$$
\begin{align}
    \pi_{k+1} | z_{k} &\sim \mathcal{D}(\lambda \odot \alpha_k) \\
    z_{k+1} &= arg\max_{j} \pi_{k+1}^{(j)}
\end{align}
$$

\begin{algorithm}[H]
    \caption{Tree quantized random walk}
    \begin{algorithmic}
        \REQUIRE $T_i$ a given tree, $(\pi_1, \lambda)$ distribution parameters
        \STATE Draw $z_{1} \sim p(\pi_1)$
        \STATE Define $Z_i^q = (z_{1})$
        \For{ $k \in \{2, \dots, N_T\}$ }
            \STATE \quad Draw a distribution $\pi_{k} | z_{k-1} \sim \mathcal{D}(\lambda \odot \alpha_{k-1})$
            \STATE \quad Mask in $\pi_{k}$ the already seen nodes in $Z_i^q$ to obtain $\tilde{\pi}_{k}$
            \STATE \quad Set $z_{k} = arg\max_{j} \tilde{\pi}_{k}^{(j)}$
            \STATE \quad Update $Z_i^q = (Z_i^q, z_k)$
        \EndFor:

        \RETURN{} $Z_i^q = (z_1, \dots, z_{N_T})$
    \end{algorithmic}

    \label{alg:tree_quantized_walk}
\end{algorithm}

Sadly, $q_{\Phi}(Z_i|X_i,T_i) \neq \delta_{\{Z_i = Z_i^q\}}$, so we do not fall back on that objective yet:
$$
\begin{align}
    ELBO(\Phi, \theta) &= \sum_{i=1}^n \log p_{\omega}(X_i | Z_i^q(T_i)) - \log \frac{q_{\Phi}(Z_i^q(T_i) | X_i)}{p_{\gamma}(Z_i^q (T_i) | T_i)} \\
    &= \sum_{i=1}^n \log p_{\omega}(X_i | Z_i^q(T_i)) + \log p_{\gamma}(Z_i^q (T_i) | T_i)
\end{align}
$$

Being a random process, the density of a moving random process is yet to be defined.