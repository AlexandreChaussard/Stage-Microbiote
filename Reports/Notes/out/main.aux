\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\babel@aux{english}{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:logo_lab}{{\caption@xref {fig:logo_lab}{ on input line 10}}{1}{}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Results to check and/or review}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{4}{section.2}\protected@file@percent }
\abx@aux@cite{0}{expectation_maximization_source}
\abx@aux@segm{0}{0}{expectation_maximization_source}
\@writefile{toc}{\contentsline {section}{\numberline {3}Expectation-Maximization}{5}{section.3}\protected@file@percent }
\newlabel{section:EM}{{3}{5}{Expectation-Maximization}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview}{5}{subsection.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Expectation-Maximization\relax }}{5}{algorithm.1}\protected@file@percent }
\newlabel{alg:em_algo}{{1}{5}{Expectation-Maximization\relax }{ALC@unique.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gaussian Mixture Linear Classifier}{6}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Framework and computations}{6}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Dataset generation}{7}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Generated samples out of $K=2$ gaussians, labelized using a logistic model (label 1 or 0). The density of the hidden gaussians is represented in the background using shades of grey (black intense, white almost 0)\relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig:samples_gmm_lc}{{1}{8}{Generated samples out of $K=2$ gaussians, labelized using a logistic model (label 1 or 0). The density of the hidden gaussians is represented in the background using shades of grey (black intense, white almost 0)\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Generated samples out of $K=2$ gaussians, using the same parameters as for figure \ref {fig:samples_gmm_lc} divided by a factor 10.\relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gmm_lc_badconditioning}{{2}{8}{Generated samples out of $K=2$ gaussians, using the same parameters as for figure \ref {fig:samples_gmm_lc} divided by a factor 10.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Dirichlet Mixture Linear Classifier}{9}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Framework and objectives}{9}{subsubsection.3.3.1}\protected@file@percent }
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Dataset generation}{10}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Generated samples out of $K=2$ dirichlet distributions, labelized using the sigmoid modelisation on $\mathbb  {P}(Y_i = 1)$.\relax }}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:dirichlet_lc_badconditioning}{{3}{11}{Generated samples out of $K=2$ dirichlet distributions, labelized using the sigmoid modelisation on $\mathbb {P}(Y_i = 1)$.\relax }{figure.caption.5}{}}
\abx@aux@cite{0}{kingma2019introduction}
\abx@aux@segm{0}{0}{kingma2019introduction}
\@writefile{toc}{\contentsline {section}{\numberline {4}Variational methods}{12}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Variational Auto-Encoders}{12}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Framework and optimization objective}{12}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Reparameterization trick}{13}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Architecture}{14}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of a VAE with Gaussian prior (wikipedia)\relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{fig:vae_architecture}{{4}{14}{Illustration of a VAE with Gaussian prior (wikipedia)\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Limits for our problem}{14}{subsubsection.4.1.4}\protected@file@percent }
\abx@aux@cite{0}{vq_vae_paper}
\abx@aux@segm{0}{0}{vq_vae_paper}
\abx@aux@cite{0}{pixel_cnn_paper}
\abx@aux@segm{0}{0}{pixel_cnn_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}VQ-VAE}{15}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Quick overview}{15}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces VQ-VAE architecture (source: Medium)\relax }}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vq_vae_architecture}{{5}{15}{VQ-VAE architecture (source: Medium)\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of the VQ-VAE: quantization layer (source: Medium)\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig:vq_vae_quantization}{{6}{15}{Architecture of the VQ-VAE: quantization layer (source: Medium)\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Framework and optimization objective}{15}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Discussion over some limiting aspects}{17}{subsubsection.4.2.3}\protected@file@percent }
\abx@aux@cite{0}{pixel_cnn_paper}
\abx@aux@segm{0}{0}{pixel_cnn_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}PixelCNN}{18}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Overview}{18}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Masked convolution on an image (largest square), the big square represents the receptive field of the black pixel, the white area is masked\relax }}{18}{figure.caption.9}\protected@file@percent }
\newlabel{fig:masked_convolution_pixel_cnn}{{7}{18}{Masked convolution on an image (largest square), the big square represents the receptive field of the black pixel, the white area is masked\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Usage in the VQ-VAE}{19}{subsubsection.4.3.2}\protected@file@percent }
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\@writefile{toc}{\contentsline {section}{\numberline {5}Microbiota analysis}{20}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Microbiota dataset}{20}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Phylogenetic tree example with abundance data (in the nodes) at each layer of the tree. Each node represents a bacterium specie at a given precision layer in the tree. From \blx@tocontentsinit {0}\cite {microbiome_deeplearning_research}.\relax }}{20}{figure.caption.10}\protected@file@percent }
\newlabel{fig:phylogenetic_tree}{{8}{20}{Phylogenetic tree example with abundance data (in the nodes) at each layer of the tree. Each node represents a bacterium specie at a given precision layer in the tree. From \cite {microbiome_deeplearning_research}.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Phylogenetic tree to image representation: opacity of the pixel relates to the abundance of the species at the given level of precision (normalized between 0 and 1). From \blx@tocontentsinit {0}\cite {microbiome_deeplearning_research}.\relax }}{20}{figure.caption.11}\protected@file@percent }
\newlabel{fig:phylogenetic_tree_to_img}{{9}{20}{Phylogenetic tree to image representation: opacity of the pixel relates to the abundance of the species at the given level of precision (normalized between 0 and 1). From \cite {microbiome_deeplearning_research}.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Taxonomy of the microbiota dataset (precision $4$)\relax }}{21}{figure.caption.12}\protected@file@percent }
\newlabel{fig:taxonomy_microbiota_p4}{{10}{21}{Taxonomy of the microbiota dataset (precision $4$)\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Taxa-abundance sample from the dataset (opacity relates the abundance value)\relax }}{21}{figure.caption.13}\protected@file@percent }
\newlabel{fig:taxa_abundance_sample_p4}{{11}{21}{Taxa-abundance sample from the dataset (opacity relates the abundance value)\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Mathematical context and objective without latent variables}{21}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Markovian model without latent variables}{22}{subsection.5.3}\protected@file@percent }
\newlabel{simple_generative_model}{{5.3}{22}{Markovian model without latent variables}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Design of the prior}{22}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Design of the posterior and maximum likelihood estimator}{24}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Optimization of the objective}{25}{subsubsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Experiments}{25}{subsubsection.5.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Global artificial tree structure\relax }}{26}{figure.caption.14}\protected@file@percent }
\newlabel{fig:tree_artificial_architecture}{{12}{26}{Global artificial tree structure\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Artificial taxa-abundance data following the global structure of figure \ref {fig:tree_artificial_architecture}. Opacity accounts for the value of the abundance.\relax }}{26}{figure.caption.15}\protected@file@percent }
\newlabel{fig:tree_artificial_sample}{{13}{26}{Artificial taxa-abundance data following the global structure of figure \ref {fig:tree_artificial_architecture}. Opacity accounts for the value of the abundance.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Log-likelihood per iteration of the optimization algorithm for $10$ models being trained with random initializations.\relax }}{27}{figure.caption.16}\protected@file@percent }
\newlabel{fig:markovian_tree_model_loglikelihood_convergence}{{14}{27}{Log-likelihood per iteration of the optimization algorithm for $10$ models being trained with random initializations.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Average log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }}{27}{figure.caption.17}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_convergence}{{15}{27}{Average log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Coordinate-wise log optimization error to the real parameters for $10$ models initialized with random parameters, $100$ samples, for $40$ steps of the fixed point algorithm.\relax }}{28}{figure.caption.18}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_estimations}{{16}{28}{Coordinate-wise log optimization error to the real parameters for $10$ models initialized with random parameters, $100$ samples, for $40$ steps of the fixed point algorithm.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Boxplots of the coordinate-wise optimization error to the real parameters of the dirichlet distribution for $10$ runs initialized with random parameters, for $40$ steps of the fixed point algorithm.\relax }}{28}{figure.caption.19}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_estimations_dirichlet_boxplot}{{17}{28}{Boxplots of the coordinate-wise optimization error to the real parameters of the dirichlet distribution for $10$ runs initialized with random parameters, for $40$ steps of the fixed point algorithm.\relax }{figure.caption.19}{}}
\abx@aux@cite{0}{momal_tree}
\abx@aux@segm{0}{0}{momal_tree}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Average log optimization error to the real parameters for $10$ models initialized with random parameters, per samples in the dataset.\relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_mean_sample}{{18}{29}{Average log optimization error to the real parameters for $10$ models initialized with random parameters, per samples in the dataset.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Average log optimization error as boxplots, for $10$ models initialized randomly, per samples in the dataset.\relax }}{29}{figure.caption.21}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_mean_sample_boxplot}{{19}{29}{Average log optimization error as boxplots, for $10$ models initialized randomly, per samples in the dataset.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Conclusions}{29}{subsubsection.5.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Taxonomy clustering: mixture model}{30}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Context}{30}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Optimization}{30}{subsubsection.5.4.2}\protected@file@percent }
\abx@aux@cite{0}{hidden_tree_markov_models}
\abx@aux@segm{0}{0}{hidden_tree_markov_models}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Experiments}{31}{subsubsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Hidden Markov Models: functional modelization}{31}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Context of Hidden Markov Models and Hidden Tree Markov Models}{31}{subsubsection.5.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Dependency graph of a HMM ($Z$ is the latent MC, $Y$ the observation)\relax }}{31}{figure.caption.22}\protected@file@percent }
\newlabel{fig:graph_dependency_hmm}{{20}{31}{Dependency graph of a HMM ($Z$ is the latent MC, $Y$ the observation)\relax }{figure.caption.22}{}}
\abx@aux@cite{0}{bottom_up_superiority_hidden_tree_markov_models}
\abx@aux@segm{0}{0}{bottom_up_superiority_hidden_tree_markov_models}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Top-down HTMM example: $Z$ represents the latent Markov tree, $X$ the observed abundance of a given bacteria in the taxonomic tree\relax }}{32}{figure.caption.23}\protected@file@percent }
\newlabel{fig:top_down_htmm_example}{{21}{32}{Top-down HTMM example: $Z$ represents the latent Markov tree, $X$ the observed abundance of a given bacteria in the taxonomic tree\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Tree generation: layer-wise Hidden Markov Model}{33}{subsubsection.5.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Layer-wise hidden markov model dependency graph: $Z$ is the HMM, $T^{(\ell )}$ is the layer $(\ell )$ of the tree\relax }}{33}{figure.caption.24}\protected@file@percent }
\newlabel{fig:layer_wise_hmm_tree}{{22}{33}{Layer-wise hidden markov model dependency graph: $Z$ is the HMM, $T^{(\ell )}$ is the layer $(\ell )$ of the tree\relax }{figure.caption.24}{}}
\abx@aux@cite{0}{bottom_up_superiority_hidden_tree_markov_models}
\abx@aux@segm{0}{0}{bottom_up_superiority_hidden_tree_markov_models}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}Abundance generation: Hidden Tree Markov Model}{34}{subsubsection.5.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Bottom-Up Hidden Tree Markov Model: general model}{34}{subsubsection.5.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Bottom-down Hidden Tree Markov Model architecture: $Z$ represents the latent space, $X$ a taxa-abundance sample. The pointed out sections and arrows represent the dependencies. Each node is marked by its layer $(\ell )$ and position within that layer $k$.\relax }}{35}{figure.caption.25}\protected@file@percent }
\newlabel{fig:bottom_up_htmm_example}{{23}{35}{Bottom-down Hidden Tree Markov Model architecture: $Z$ represents the latent space, $X$ a taxa-abundance sample. The pointed out sections and arrows represent the dependencies. Each node is marked by its layer $(\ell )$ and position within that layer $k$.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5}Bottom-Up Hidden Tree Markov Model: low sample regime}{36}{subsubsection.5.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{38}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Microbiota analysis}{38}{subsection.6.1}\protected@file@percent }
\newlabel{proposition:bernoulli_tree_prior}{{6.1}{38}{}{theorem.6.1}{}}
\newlabel{proposition:mle_bernoulli_tree_prior}{{6.2}{39}{MLE of the Bernoulli tree prior}{theorem.6.2}{}}
\newlabel{eq:prior_transition_objective}{{1}{39}{MLE of the Bernoulli tree prior}{equation.6.1}{}}
\newlabel{proposition:law_of_aX}{{6.3}{39}{Law of $aX$}{theorem.6.3}{}}
\newlabel{proposition:abundance_posterior_bernoulli_tree}{{6.4}{40}{Abundance distribution conditionally to the trees}{theorem.6.4}{}}
\newlabel{proposition:MLE_abundance_bernoulli_tree}{{6.5}{40}{MLE of the abundance distribution a posteriori}{theorem.6.5}{}}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\newlabel{proposition:EM_taxonomy_clustering}{{6.6}{41}{Expectation-Maximization for Taxonomy Clustering model}{theorem.6.6}{}}
\newlabel{eq:objective_latent_gamma_c_microbiota_clustering}{{2}{44}{Microbiota analysis}{equation.6.2}{}}
\newlabel{proposition:likelihood_layer_wise_hmm_tree}{{6.7}{45}{Likelihood of layer-wise hidden markov model tree generator}{theorem.6.7}{}}
\newlabel{proposition:em_layer_wise_hmm_tree}{{6.8}{46}{Expectation-Maximization for layer-wise hidden markov model for tree generation}{theorem.6.8}{}}
\newlabel{proposition:likelihood_bottom_up_htmm_taxaabundance_data}{{6.9}{48}{Joint distribution of Bottom-Up Hidden Tree Markov Model applied to top-down markov tree data}{theorem.6.9}{}}
\newlabel{proposition:EM_bottom_up_HTMM_dirichlet}{{6.10}{48}{Expectation-Maximization for Bottom-Up Hidden Tree Markov Model with dirichlet modelization}{theorem.6.10}{}}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\newlabel{proposition:likelihood_underparameterized_bottomup_htmm}{{6.11}{52}{Likelihood under-parameterized Bottom-Up HTMM}{theorem.6.11}{}}
\newlabel{proposition:EM_underparameterized_bottomup_HTMM}{{6.12}{53}{}{theorem.6.12}{}}
\abx@aux@nociteall
\@writefile{toc}{\contentsline {section}{\numberline {7}Bibliography}{55}{section.7}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{AFF9C9FC08F1BEF401F4D323ABB18CA5}
\abx@aux@defaultrefcontext{0}{bottom_up_superiority_hidden_tree_markov_models}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{expectation_maximization_source}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{hidden_tree_markov_models}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{microbiome_deeplearning_research}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{kingma2019introduction}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{dirichlet_digamma_trick}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{momal_tree}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{pixel_cnn_paper}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{vq_vae_paper}{nty/global//global/global}
\gdef \@abspage@last{56}
