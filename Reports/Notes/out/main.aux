\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\babel@aux{english}{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:logo_ipp}{{\caption@xref {fig:logo_ipp}{ on input line 9}}{1}{}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\abx@aux@cite{0}{expectation_maximization_source}
\abx@aux@segm{0}{0}{expectation_maximization_source}
\@writefile{toc}{\contentsline {section}{\numberline {2}Expectation-Maximization}{4}{section.2}\protected@file@percent }
\newlabel{section:EM}{{2}{4}{Expectation-Maximization}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Overview}{4}{subsection.2.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Expectation-Maximization\relax }}{4}{algorithm.1}\protected@file@percent }
\newlabel{alg:em_algo}{{1}{4}{Expectation-Maximization\relax }{ALC@unique.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gaussian Mixture Linear Classifier}{5}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Framework and computations}{5}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Dataset generation}{6}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Generated samples out of $K=2$ gaussians, labelized using a logistic model (label 1 or 0). The density of the hidden gaussians is represented in the background using shades of grey (black intense, white almost 0)\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:samples_gmm_lc}{{1}{7}{Generated samples out of $K=2$ gaussians, labelized using a logistic model (label 1 or 0). The density of the hidden gaussians is represented in the background using shades of grey (black intense, white almost 0)\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Generated samples out of $K=2$ gaussians, using the same parameters as for figure \ref {fig:samples_gmm_lc} divided by a factor 10.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gmm_lc_badconditioning}{{2}{7}{Generated samples out of $K=2$ gaussians, using the same parameters as for figure \ref {fig:samples_gmm_lc} divided by a factor 10.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Dirichlet Mixture Linear Classifier}{8}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Framework and objectives}{8}{subsubsection.2.3.1}\protected@file@percent }
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Dataset generation}{9}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Generated samples out of $K=2$ dirichlet distributions, labelized using the sigmoid modelisation on $\mathbb  {P}(Y_i = 1)$.\relax }}{10}{figure.caption.5}\protected@file@percent }
\newlabel{fig:dirichlet_lc_badconditioning}{{3}{10}{Generated samples out of $K=2$ dirichlet distributions, labelized using the sigmoid modelisation on $\mathbb {P}(Y_i = 1)$.\relax }{figure.caption.5}{}}
\abx@aux@cite{0}{kingma2019introduction}
\abx@aux@segm{0}{0}{kingma2019introduction}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational methods}{11}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Variational Auto-Encoders}{11}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Framework and optimization objective}{11}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Reparameterization trick}{12}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Architecture}{13}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of a VAE with Gaussian prior (wikipedia)\relax }}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:vae_architecture}{{4}{13}{Illustration of a VAE with Gaussian prior (wikipedia)\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Limits for our problem}{13}{subsubsection.3.1.4}\protected@file@percent }
\abx@aux@cite{0}{vq_vae_paper}
\abx@aux@segm{0}{0}{vq_vae_paper}
\abx@aux@cite{0}{pixel_cnn_paper}
\abx@aux@segm{0}{0}{pixel_cnn_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}VQ-VAE}{14}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Quick overview}{14}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces VQ-VAE architecture (source: Medium)\relax }}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vq_vae_architecture}{{5}{14}{VQ-VAE architecture (source: Medium)\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of the VQ-VAE: quantization layer (source: Medium)\relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:vq_vae_quantization}{{6}{14}{Architecture of the VQ-VAE: quantization layer (source: Medium)\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Framework and optimization objective}{14}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Discussion over some limiting aspects}{16}{subsubsection.3.2.3}\protected@file@percent }
\abx@aux@cite{0}{pixel_cnn_paper}
\abx@aux@segm{0}{0}{pixel_cnn_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}PixelCNN}{17}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Overview}{17}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Masked convolution on an image (largest square), the big square represents the receptive field of the black pixel, the white area is masked\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{fig:masked_convolution_pixel_cnn}{{7}{17}{Masked convolution on an image (largest square), the big square represents the receptive field of the black pixel, the white area is masked\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Usage in the VQ-VAE}{18}{subsubsection.3.3.2}\protected@file@percent }
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\@writefile{toc}{\contentsline {section}{\numberline {4}Microbiota analysis}{19}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Microbiota dataset}{19}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Phylogenetic tree example with abundance data (in the nodes) at each layer of the tree. Each node represents a bacterium specie at a given precision layer in the tree. From \blx@tocontentsinit {0}\cite {microbiome_deeplearning_research}.\relax }}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig:phylogenetic_tree}{{8}{19}{Phylogenetic tree example with abundance data (in the nodes) at each layer of the tree. Each node represents a bacterium specie at a given precision layer in the tree. From \cite {microbiome_deeplearning_research}.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Phylogenetic tree to image representation: opacity of the pixel relates to the abundance of the species at the given level of precision (normalized between 0 and 1). From \blx@tocontentsinit {0}\cite {microbiome_deeplearning_research}.\relax }}{19}{figure.caption.11}\protected@file@percent }
\newlabel{fig:phylogenetic_tree_to_img}{{9}{19}{Phylogenetic tree to image representation: opacity of the pixel relates to the abundance of the species at the given level of precision (normalized between 0 and 1). From \cite {microbiome_deeplearning_research}.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Taxonomy of the microbiota dataset (precision $4$)\relax }}{20}{figure.caption.12}\protected@file@percent }
\newlabel{fig:taxonomy_microbiota_p4}{{10}{20}{Taxonomy of the microbiota dataset (precision $4$)\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Taxa-abundance sample from the dataset (opacity relates the abundance value)\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:taxa_abundance_sample_p4}{{11}{20}{Taxa-abundance sample from the dataset (opacity relates the abundance value)\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Mathematical context and objective without latent variables}{20}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Markovian model without latent variables}{21}{subsection.4.3}\protected@file@percent }
\newlabel{simple_generative_model}{{4.3}{21}{Markovian model without latent variables}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Design of the prior}{21}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Design of the posterior and maximum likelihood estimator}{23}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Optimization of the objective}{24}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Experiments}{24}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Global artificial tree structure\relax }}{25}{figure.caption.14}\protected@file@percent }
\newlabel{fig:tree_artificial_architecture}{{12}{25}{Global artificial tree structure\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Artificial taxa-abundance data following the global structure of figure \ref {fig:tree_artificial_architecture}. Opacity accounts for the value of the abundance.\relax }}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:tree_artificial_sample}{{13}{25}{Artificial taxa-abundance data following the global structure of figure \ref {fig:tree_artificial_architecture}. Opacity accounts for the value of the abundance.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Log-likelihood per iteration of the optimization algorithm for $10$ models being trained with random initializations.\relax }}{26}{figure.caption.16}\protected@file@percent }
\newlabel{fig:markovian_tree_model_loglikelihood_convergence}{{14}{26}{Log-likelihood per iteration of the optimization algorithm for $10$ models being trained with random initializations.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Average log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }}{26}{figure.caption.17}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_convergence}{{15}{26}{Average log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Coordinate-wise log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }}{27}{figure.caption.18}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_estimations}{{16}{27}{Coordinate-wise log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }{figure.caption.18}{}}
\abx@aux@cite{0}{momal_tree}
\abx@aux@segm{0}{0}{momal_tree}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Average log optimization error to the real parameters for $10$ models initialized with random parameters, per samples in the dataset.\relax }}{28}{figure.caption.19}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_mean_sample}{{17}{28}{Average log optimization error to the real parameters for $10$ models initialized with random parameters, per samples in the dataset.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Conclusions}{28}{subsubsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Taxonomy clustering: mixture model}{29}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Context}{29}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Optimization}{29}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Experiments}{29}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Hidden Markov Models: functional modelization}{29}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Context of Hidden Markov Models and Hidden Tree Markov Models}{29}{subsubsection.4.5.1}\protected@file@percent }
\abx@aux@cite{0}{hidden_tree_markov_models}
\abx@aux@segm{0}{0}{hidden_tree_markov_models}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Dependency graph of a HMM ($Z$ is the latent MC, $Y$ the observation)\relax }}{30}{figure.caption.20}\protected@file@percent }
\newlabel{fig:graph_dependency_hmm}{{18}{30}{Dependency graph of a HMM ($Z$ is the latent MC, $Y$ the observation)\relax }{figure.caption.20}{}}
\abx@aux@cite{0}{bottom_up_superiority_hidden_tree_markov_models}
\abx@aux@segm{0}{0}{bottom_up_superiority_hidden_tree_markov_models}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Top-down HTMM example: $Z$ represents the latent Markov tree, $X$ the observed abundance of a given bacteria in the taxonomic tree\relax }}{31}{figure.caption.21}\protected@file@percent }
\newlabel{fig:top_down_htmm_example}{{19}{31}{Top-down HTMM example: $Z$ represents the latent Markov tree, $X$ the observed abundance of a given bacteria in the taxonomic tree\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Tree generation: layer-wise Hidden Markov Model}{31}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Layer-wise hidden markov model dependency graph: $Z$ is the HMM, $T^{(\ell )}$ is the layer $(\ell )$ of the tree\relax }}{32}{figure.caption.22}\protected@file@percent }
\newlabel{fig:layer_wise_hmm_tree}{{20}{32}{Layer-wise hidden markov model dependency graph: $Z$ is the HMM, $T^{(\ell )}$ is the layer $(\ell )$ of the tree\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Abundance generation: Hidden Tree Markov Model}{33}{subsubsection.4.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendix}{34}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Microbiota analysis}{34}{subsection.5.1}\protected@file@percent }
\newlabel{proposition:bernoulli_tree_prior}{{5.1}{34}{}{theorem.5.1}{}}
\newlabel{proposition:mle_bernoulli_tree_prior}{{5.2}{35}{MLE of the Bernoulli tree prior}{theorem.5.2}{}}
\newlabel{eq:prior_transition_objective}{{1}{35}{MLE of the Bernoulli tree prior}{equation.5.1}{}}
\newlabel{proposition:law_of_aX}{{5.3}{35}{Law of $aX$}{theorem.5.3}{}}
\newlabel{proposition:abundance_posterior_bernoulli_tree}{{5.4}{36}{Abundance distribution conditionally to the trees}{theorem.5.4}{}}
\newlabel{MLE_abundance_bernoulli_tree}{{5.5}{36}{MLE of the abundance distribution a posteriori}{theorem.5.5}{}}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\newlabel{proposition:EM_taxonomy_clustering}{{5.6}{37}{Expectation-Maximization for Taxonomy Clustering model}{theorem.5.6}{}}
\newlabel{eq:objective_latent_gamma_c_microbiota_clustering}{{2}{40}{Microbiota analysis}{equation.5.2}{}}
\newlabel{proposition:likelihood_layer_wise_hmm_tree}{{5.7}{41}{Likelihood of layer-wise hidden markov model tree generator}{theorem.5.7}{}}
\newlabel{proposition:em_layer_wise_hmm_tree}{{5.8}{42}{Expectation-Maximization for layer-wise hidden markov model for tree generation}{theorem.5.8}{}}
\abx@aux@nociteall
\@writefile{toc}{\contentsline {section}{\numberline {6}Bibliography}{44}{section.6}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{AFF9C9FC08F1BEF401F4D323ABB18CA5}
\abx@aux@defaultrefcontext{0}{bottom_up_superiority_hidden_tree_markov_models}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{expectation_maximization_source}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{hidden_tree_markov_models}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{microbiome_deeplearning_research}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{kingma2019introduction}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{dirichlet_digamma_trick}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{momal_tree}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{pixel_cnn_paper}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{vq_vae_paper}{nty/global//global/global}
\gdef \@abspage@last{45}
