\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\babel@aux{english}{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:logo_ipp}{{\caption@xref {fig:logo_ipp}{ on input line 9}}{1}{}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\abx@aux@cite{0}{expectation_maximization_source}
\abx@aux@segm{0}{0}{expectation_maximization_source}
\@writefile{toc}{\contentsline {section}{\numberline {2}Expectation-Maximization}{3}{section.2}\protected@file@percent }
\newlabel{section:EM}{{2}{3}{Expectation-Maximization}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Overview}{3}{subsection.2.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Expectation-Maximization\relax }}{3}{algorithm.1}\protected@file@percent }
\newlabel{alg:em_algo}{{1}{3}{Expectation-Maximization\relax }{ALC@unique.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gaussian Mixture Linear Classifier}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Framework and computations}{4}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Dataset generation}{5}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Generated samples out of $K=2$ gaussians, labelized using a logistic model (label 1 or 0). The density of the hidden gaussians is represented in the background using shades of grey (black intense, white almost 0)\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:samples_gmm_lc}{{1}{6}{Generated samples out of $K=2$ gaussians, labelized using a logistic model (label 1 or 0). The density of the hidden gaussians is represented in the background using shades of grey (black intense, white almost 0)\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Generated samples out of $K=2$ gaussians, using the same parameters as for figure \ref {fig:samples_gmm_lc} divided by a factor 10.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gmm_lc_badconditioning}{{2}{6}{Generated samples out of $K=2$ gaussians, using the same parameters as for figure \ref {fig:samples_gmm_lc} divided by a factor 10.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Dirichlet Mixture Linear Classifier}{7}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Framework and objectives}{7}{subsubsection.2.3.1}\protected@file@percent }
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Dataset generation}{8}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Generated samples out of $K=2$ dirichlet distributions, labelized using the sigmoid modelisation on $\mathbb  {P}(Y_i = 1)$.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:dirichlet_lc_badconditioning}{{3}{9}{Generated samples out of $K=2$ dirichlet distributions, labelized using the sigmoid modelisation on $\mathbb {P}(Y_i = 1)$.\relax }{figure.caption.5}{}}
\abx@aux@cite{0}{kingma2019introduction}
\abx@aux@segm{0}{0}{kingma2019introduction}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational methods}{10}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Variational Auto-Encoders}{10}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Framework and optimization objective}{10}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Reparameterization trick}{11}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Architecture}{12}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of a VAE with Gaussian prior (wikipedia)\relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:vae_architecture}{{4}{12}{Illustration of a VAE with Gaussian prior (wikipedia)\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Limits for our problem}{12}{subsubsection.3.1.4}\protected@file@percent }
\abx@aux@cite{0}{vq_vae_paper}
\abx@aux@segm{0}{0}{vq_vae_paper}
\abx@aux@cite{0}{pixel_cnn_paper}
\abx@aux@segm{0}{0}{pixel_cnn_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}VQ-VAE}{13}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Quick overview}{13}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces VQ-VAE architecture (source: Medium)\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vq_vae_architecture}{{5}{13}{VQ-VAE architecture (source: Medium)\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of the VQ-VAE: quantization layer (source: Medium)\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:vq_vae_quantization}{{6}{13}{Architecture of the VQ-VAE: quantization layer (source: Medium)\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Framework and optimization objective}{13}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Discussion over some limiting aspects}{15}{subsubsection.3.2.3}\protected@file@percent }
\abx@aux@cite{0}{pixel_cnn_paper}
\abx@aux@segm{0}{0}{pixel_cnn_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}PixelCNN}{16}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Overview}{16}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Masked convolution on an image (largest square), the big square represents the receptive field of the black pixel, the white area is masked\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:masked_convolution_pixel_cnn}{{7}{16}{Masked convolution on an image (largest square), the big square represents the receptive field of the black pixel, the white area is masked\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Usage in the VQ-VAE}{17}{subsubsection.3.3.2}\protected@file@percent }
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\abx@aux@cite{0}{microbiome_deeplearning_research}
\abx@aux@segm{0}{0}{microbiome_deeplearning_research}
\@writefile{toc}{\contentsline {section}{\numberline {4}Microbiota analysis}{18}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Microbiota dataset}{18}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Phylogenetic tree example with abundance data (in the nodes) at each layer of the tree. Each node represents a bacterium specie at a given precision layer in the tree. From \blx@tocontentsinit {0}\cite {microbiome_deeplearning_research}.\relax }}{18}{figure.caption.10}\protected@file@percent }
\newlabel{fig:phylogenetic_tree}{{8}{18}{Phylogenetic tree example with abundance data (in the nodes) at each layer of the tree. Each node represents a bacterium specie at a given precision layer in the tree. From \cite {microbiome_deeplearning_research}.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Phylogenetic tree to image representation: opacity of the pixel relates to the abundance of the species at the given level of precision (normalized between 0 and 1). From \blx@tocontentsinit {0}\cite {microbiome_deeplearning_research}.\relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{fig:phylogenetic_tree_to_img}{{9}{18}{Phylogenetic tree to image representation: opacity of the pixel relates to the abundance of the species at the given level of precision (normalized between 0 and 1). From \cite {microbiome_deeplearning_research}.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Taxonomy of the microbiota dataset (precision $4$)\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:taxonomy_microbiota_p4}{{10}{19}{Taxonomy of the microbiota dataset (precision $4$)\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Taxa-abundance sample from the dataset (opacity relates the abundance value)\relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:taxa_abundance_sample_p4}{{11}{19}{Taxa-abundance sample from the dataset (opacity relates the abundance value)\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Mathematical context and objective without latent variables}{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Markovian model without latent variables}{20}{subsection.4.3}\protected@file@percent }
\newlabel{simple_generative_model}{{4.3}{20}{Markovian model without latent variables}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Design of the prior}{20}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Design of the posterior and maximum likelihood estimator}{22}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Optimization of the objective}{23}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Experiments}{23}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Global artificial tree structure\relax }}{24}{figure.caption.14}\protected@file@percent }
\newlabel{fig:tree_artificial_architecture}{{12}{24}{Global artificial tree structure\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Artificial taxa-abundance data following the global structure of figure \ref {fig:tree_artificial_architecture}. Opacity accounts for the value of the abundance.\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{fig:tree_artificial_sample}{{13}{24}{Artificial taxa-abundance data following the global structure of figure \ref {fig:tree_artificial_architecture}. Opacity accounts for the value of the abundance.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Log-likelihood per iteration of the optimization algorithm for $10$ models being trained with random initializations.\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:markovian_tree_model_loglikelihood_convergence}{{14}{25}{Log-likelihood per iteration of the optimization algorithm for $10$ models being trained with random initializations.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Average log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }}{25}{figure.caption.17}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_convergence}{{15}{25}{Average log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Coordinate-wise log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }}{26}{figure.caption.18}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_estimations}{{16}{26}{Coordinate-wise log optimization error to the real parameters for $10$ models initialized with random parameters, per iteration.\relax }{figure.caption.18}{}}
\abx@aux@cite{0}{momal_tree}
\abx@aux@segm{0}{0}{momal_tree}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Average log optimization error to the real parameters for $10$ models initialized with random parameters, per samples in the dataset.\relax }}{27}{figure.caption.19}\protected@file@percent }
\newlabel{fig:markovian_tree_model_error_mean_sample}{{17}{27}{Average log optimization error to the real parameters for $10$ models initialized with random parameters, per samples in the dataset.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Conclusions}{27}{subsubsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Taxonomy clustering: mixture model}{28}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Context}{28}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Optimization}{28}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendix}{29}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Microbiota analysis}{29}{subsection.5.1}\protected@file@percent }
\newlabel{proposition:bernoulli_tree_prior}{{5.1}{29}{}{theorem.5.1}{}}
\newlabel{proposition:mle_bernoulli_tree_prior}{{5.2}{30}{MLE of the Bernoulli tree prior}{theorem.5.2}{}}
\newlabel{eq:prior_transition_objective}{{1}{30}{MLE of the Bernoulli tree prior}{equation.5.1}{}}
\newlabel{proposition:law_of_aX}{{5.3}{30}{Law of $aX$}{theorem.5.3}{}}
\newlabel{proposition:abundance_posterior_bernoulli_tree}{{5.4}{31}{Abundance distribution conditionally to the trees}{theorem.5.4}{}}
\newlabel{MLE_abundance_bernoulli_tree}{{5.5}{31}{MLE of the abundance distribution a posteriori}{theorem.5.5}{}}
\abx@aux@cite{0}{dirichlet_digamma_trick}
\abx@aux@segm{0}{0}{dirichlet_digamma_trick}
\newlabel{proposition:EM_taxonomy_clustering}{{5.6}{32}{Expectation-Maximization for Taxonomy Clustering model}{theorem.5.6}{}}
\abx@aux@nociteall
\@writefile{toc}{\contentsline {section}{\numberline {6}Bibliography}{34}{section.6}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{065E4C84D50E20897ECA71C3645DFBE4}
\abx@aux@defaultrefcontext{0}{expectation_maximization_source}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{microbiome_deeplearning_research}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{kingma2019introduction}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{dirichlet_digamma_trick}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{momal_tree}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{pixel_cnn_paper}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{vq_vae_paper}{nty/global//global/global}
\gdef \@abspage@last{35}
